{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import r2_score\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "train_df  = pd.read_csv('./input/train.csv')\n",
    "test_df  = pd.read_csv('./input/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4209, 378)\n",
      "(4209, 377)\n"
     ]
    }
   ],
   "source": [
    "print(train_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get train_y, test ids and unite datasets to perform\n",
    "train_y = train_df['y']\n",
    "train_df.drop('y', axis = 1, inplace = True)\n",
    "test_ids = test_df.ID.values\n",
    "all_df = pd.concat([train_df,test_df], axis = 0)\n",
    "\n",
    "# ...one hot encoding of categorical variables\n",
    "categorical =  [\"X0\", \"X1\", \"X2\", \"X3\", \"X4\", \"X5\", \"X6\", \"X8\"]\n",
    "for f in categorical:\n",
    "    dummies = pd.get_dummies(all_df[f], prefix = f, prefix_sep = '_')\n",
    "    all_df = pd.concat([all_df, dummies], axis = 1)\n",
    "\n",
    "# drop original categorical features\n",
    "all_df.drop(categorical, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get feature dataset for test and training        \n",
    "train_X = all_df.drop([\"ID\"], axis=1).iloc[:len(train_df),:]\n",
    "test_X = all_df.drop([\"ID\"], axis=1).iloc[len(train_df):,:]\n",
    "predictors = train_X.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# transfrom y to loy(y+1) since it is skewed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#save_train_df = pd.concat([train_X,train_y],axis=1)\n",
    "#save_train_df.to_csv('./input/processed_train.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test_X.to_csv('./input/processed_test.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_train_y = np.log(train_y+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD8CAYAAACRkhiPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAEk9JREFUeJzt3X+MHPd93vH3U7JR/SOOpepCMCRdEgGTlBISJL4yahy0\nilVUdOWaKmAIdJOadQUTgdnULQI4YgrEBQoCClqkrttSBSG7ohHHBOHYEONYjlm2rlo4snryL4mU\nGV1DSSQjiec4rdoEUEr50z92Em9P92O5c9o95vt+AYed/c53dp47YO+5mdndS1UhSWrTn5t2AEnS\n9FgCktQwS0CSGmYJSFLDLAFJapglIEkNswQkqWGWgCQ1zBKQpIZtnHaA1dx44421ffv2aceQpGvK\nY4899s2qmllt3rovge3btzM3NzftGJJ0TUnyzCjzPB0kSQ2zBCSpYZaAJDVs1RJI8tEkl5M8scS6\nn09SSW4cGjuUZD7JuSS3D42/Ocnj3boPJ8nafRuSpHGMciTwALBn8WCSbcDfBJ4dGtsF7ANu6rY5\nkmRDt/o+4L3Azu7rFY8pSZqsVUugqh4GvrXEqn8FfAAY/q80e4HjVfVSVZ0H5oHdSTYDb6iqR2rw\nX2w+BtzZO70kqZexrgkk2QtcqqqvLVq1BbgwdP9iN7alW148vtzjH0gyl2RuYWFhnIiSpBFcdQkk\neS3wi8AvrX2cgao6WlWzVTU7M7Pqex0kSWMa581i3w/sAL7WXdvdCnw5yW7gErBtaO7WbuxSt7x4\nXJI0RVddAlX1OPC9f3I/ydPAbFV9M8lJ4NeS/ArwfQwuAD9aVS8neTHJLcCXgHcD/2YtvoH1aPs9\nvzm1fT997x1T27eka88oLxH9BPDbwA8muZjk7uXmVtUZ4ARwFvgccLCqXu5Wvw+4n8HF4v8BPNQz\nuySpp1WPBKrqXaus377o/mHg8BLz5oCbrzKfJOlV5DuGJalhloAkNcwSkKSGWQKS1DBLQJIaZglI\nUsMsAUlqmCUgSQ2zBCSpYZaAJDXMEpCkhlkCktSwcf6fgNaxaX2MtR9hLV2bPBKQpIZZApLUMEtA\nkhpmCUhSwywBSWqYJSBJDbMEJKlhq5ZAko8muZzkiaGxf5HkG0m+nuTTSd44tO5Qkvkk55LcPjT+\n5iSPd+s+nCRr/+1Ikq7GKEcCDwB7Fo2dAm6uqh8Gfgc4BJBkF7APuKnb5kiSDd029wHvBXZ2X4sf\nU5I0YauWQFU9DHxr0djnq+pKd/cRYGu3vBc4XlUvVdV5YB7YnWQz8IaqeqSqCvgYcOdafROSpPGs\nxTWBfwA81C1vAS4MrbvYjW3plhePS5KmqNdnByX5p8AV4ONrE+dPH/cAcADgTW9601o+tF4l0/rM\nIvBzi6Q+xj4SSPL3gbcDP92d4gG4BGwbmra1G7vEd04ZDY8vqaqOVtVsVc3OzMyMG1GStIqxSiDJ\nHuADwDuq6o+GVp0E9iW5LskOBheAH62q54AXk9zSvSro3cCDPbNLknpa9XRQkk8AtwI3JrkIfJDB\nq4GuA051r/R8pKp+tqrOJDkBnGVwmuhgVb3cPdT7GLzS6DUMriE8hCRpqlYtgap61xLDH1lh/mHg\n8BLjc8DNV5VOkvSq8h3DktQwS0CSGmYJSFLDLAFJapglIEkNswQkqWGWgCQ1zBKQpIZZApLUMEtA\nkhpmCUhSwywBSWqYJSBJDbMEJKlhloAkNcwSkKSGWQKS1DBLQJIaZglIUsMsAUlqmCUgSQ1btQSS\nfDTJ5SRPDI3dkORUkqe62+uH1h1KMp/kXJLbh8bfnOTxbt2Hk2Ttvx1J0tUY5UjgAWDPorF7gNNV\ntRM43d0nyS5gH3BTt82RJBu6be4D3gvs7L4WP6YkacJWLYGqehj41qLhvcCxbvkYcOfQ+PGqeqmq\nzgPzwO4km4E3VNUjVVXAx4a2kSRNybjXBDZV1XPd8vPApm55C3BhaN7FbmxLt7x4XJI0Rb0vDHd/\n2dcaZPlTSQ4kmUsyt7CwsJYPLUkaMm4JvNCd4qG7vdyNXwK2Dc3b2o1d6pYXjy+pqo5W1WxVzc7M\nzIwZUZK0mnFL4CSwv1veDzw4NL4vyXVJdjC4APxod+roxSS3dK8KevfQNpKkKdm42oQknwBuBW5M\nchH4IHAvcCLJ3cAzwF0AVXUmyQngLHAFOFhVL3cP9T4GrzR6DfBQ9yVJmqJVS6Cq3rXMqtuWmX8Y\nOLzE+Bxw81WlkyS9qnzHsCQ1zBKQpIZZApLUMEtAkhpmCUhSwywBSWqYJSBJDbMEJKlhloAkNcwS\nkKSGWQKS1DBLQJIaZglIUsMsAUlqmCUgSQ2zBCSpYZaAJDXMEpCkhlkCktQwS0CSGmYJSFLDepVA\nkn+S5EySJ5J8IslfSHJDklNJnupurx+afyjJfJJzSW7vH1+S1MfYJZBkC/CPgNmquhnYAOwD7gFO\nV9VO4HR3nyS7uvU3AXuAI0k29IsvSeqj7+mgjcBrkmwEXgv8HrAXONatPwbc2S3vBY5X1UtVdR6Y\nB3b33L8kqYexS6CqLgH/EngWeA74X1X1eWBTVT3XTXse2NQtbwEuDD3ExW5MkjQlfU4HXc/gr/sd\nwPcBr0vyM8NzqqqAGuOxDySZSzK3sLAwbkRJ0ir6nA76G8D5qlqoqv8LfAr4CeCFJJsButvL3fxL\nwLah7bd2Y69QVUeraraqZmdmZnpElCStpE8JPAvckuS1SQLcBjwJnAT2d3P2Aw92yyeBfUmuS7ID\n2Ak82mP/kqSeNo67YVV9KckngS8DV4CvAEeB1wMnktwNPAPc1c0/k+QEcLabf7CqXu6ZX5LUw9gl\nAFBVHwQ+uGj4JQZHBUvNPwwc7rNPSdLa8R3DktQwS0CSGmYJSFLDLAFJapglIEkNswQkqWGWgCQ1\nzBKQpIZZApLUMEtAkhpmCUhSwywBSWqYJSBJDbMEJKlhloAkNcwSkKSGWQKS1DBLQJIaZglIUsMs\nAUlqmCUgSQ3rVQJJ3pjkk0m+keTJJH81yQ1JTiV5qru9fmj+oSTzSc4lub1/fElSH32PBP418Lmq\n+iHgR4AngXuA01W1Ezjd3SfJLmAfcBOwBziSZEPP/UuSehi7BJJ8D/DXgI8AVNUfV9X/BPYCx7pp\nx4A7u+W9wPGqeqmqzgPzwO5x9y9J6q/PkcAOYAH4D0m+kuT+JK8DNlXVc92c54FN3fIW4MLQ9he7\nMUnSlPQpgY3AjwH3VdWPAn9Id+rnT1RVAXW1D5zkQJK5JHMLCws9IkqSVtKnBC4CF6vqS939TzIo\nhReSbAbobi936y8B24a239qNvUJVHa2q2aqanZmZ6RFRkrSSsUugqp4HLiT5wW7oNuAscBLY343t\nBx7slk8C+5Jcl2QHsBN4dNz9S5L629hz+58DPp7ku4DfBd7DoFhOJLkbeAa4C6CqziQ5waAorgAH\nq+rlnvuXJPXQqwSq6qvA7BKrbltm/mHgcJ99SpLWju8YlqSGWQKS1DBLQJIaZglIUsMsAUlqmCUg\nSQ2zBCSpYZaAJDXMEpCkhlkCktQwS0CSGmYJSFLDLAFJapglIEkNswQkqWGWgCQ1zBKQpIZZApLU\nMEtAkhpmCUhSwywBSWpY7xJIsiHJV5J8prt/Q5JTSZ7qbq8fmnsoyXySc0lu77tvSVI/a3Ek8H7g\nyaH79wCnq2oncLq7T5JdwD7gJmAPcCTJhjXYvyRpTL1KIMlW4A7g/qHhvcCxbvkYcOfQ+PGqeqmq\nzgPzwO4++5ck9dP3SOBDwAeAbw+Nbaqq57rl54FN3fIW4MLQvIvdmCRpSsYugSRvBy5X1WPLzamq\nAmqMxz6QZC7J3MLCwrgRJUmr6HMk8BbgHUmeBo4Db03yq8ALSTYDdLeXu/mXgG1D22/txl6hqo5W\n1WxVzc7MzPSIKElaydglUFWHqmprVW1ncMH3P1XVzwAngf3dtP3Ag93ySWBfkuuS7AB2Ao+OnVyS\n1NvGV+Ex7wVOJLkbeAa4C6CqziQ5AZwFrgAHq+rlV2H/kqQRrUkJVNUXgC90y78P3LbMvMPA4bXY\npySpP98xLEkNswQkqWGWgCQ1zBKQpIZZApLUMEtAkhpmCUhSwywBSWqYJSBJDbMEJKlhloAkNcwS\nkKSGWQKS1DBLQJIaZglIUsMsAUlqmCUgSQ2zBCSpYZaAJDXMEpCkhlkCktSwsUsgybYk/znJ2SRn\nkry/G78hyakkT3W31w9tcyjJfJJzSW5fi29AkjS+PkcCV4Cfr6pdwC3AwSS7gHuA01W1Ezjd3adb\ntw+4CdgDHEmyoU94SVI/Y5dAVT1XVV/ulv838CSwBdgLHOumHQPu7Jb3Aser6qWqOg/MA7vH3b8k\nqb81uSaQZDvwo8CXgE1V9Vy36nlgU7e8BbgwtNnFbkySNCW9SyDJ64FfB/5xVb04vK6qCqgxHvNA\nkrkkcwsLC30jSpKW0asEkvx5BgXw8ar6VDf8QpLN3frNwOVu/BKwbWjzrd3YK1TV0aqararZmZmZ\nPhElSSvo8+qgAB8BnqyqXxladRLY3y3vBx4cGt+X5LokO4CdwKPj7l+S1N/GHtu+Bfh7wONJvtqN\n/SJwL3Aiyd3AM8BdAFV1JskJ4CyDVxYdrKqXe+xfktTT2CVQVf8NyDKrb1tmm8PA4XH3KUlaW75j\nWJIaZglIUsP6XBNY97bf85vTjiBJ69qf6RJQG6ZV9k/fe8dU9iutJU8HSVLDLAFJapglIEkNswQk\nqWGWgCQ1zBKQpIZZApLUMEtAkhpmCUhSwywBSWqYJSBJDbMEJKlhloAkNcwSkKSGWQKS1DBLQJIa\n5j+VkcY0zf9c5z+00VqZ+JFAkj1JziWZT3LPpPcvSfqOiZZAkg3AvwPeBuwC3pVk1yQzSJK+Y9JH\nAruB+ar63ar6Y+A4sHfCGSRJnUlfE9gCXBi6fxH48QlnkK5507oe4bWIP3vW5YXhJAeAA93d/5Pk\n3DTzrOJG4JvTDjGmazk7XNv5r8ns+WXgGs3eaSn7Xxpl0qRL4BKwbej+1m7s/1NVR4GjkwrVR5K5\nqpqddo5xXMvZ4drOb/bpMPsrTfqawH8HdibZkeS7gH3AyQlnkCR1JnokUFVXkvxD4LeADcBHq+rM\nJDNIkr5j4tcEquqzwGcnvd9X0TVx2moZ13J2uLbzm306zL5IqurVeFxJ0jXAzw6SpIZZAlchyYYk\nX0nymRXm/JUkV5K8c5LZVrNa9iS3JvlqkjNJ/suk861kpexJvifJbyT5Wpf9PdPIuJQkTyd5vPu5\nzi2xPkk+3H2EyteT/Ng0ci5nhPw/3eV+PMkXk/zINHIuZbXsQ/PW3fN1lOxr+Xxdl+8TWMfeDzwJ\nvGGpld3HYvwy8PlJhhrRstmTvBE4AuypqmeTfO+kw61ipZ/7QeBsVf3tJDPAuSQf796Rvh78VFUt\n99rutwE7u68fB+5j/b15cqX854G/XlV/kORtDM5Zr6f8K2Vf78/XZbOv9fPVI4ERJdkK3AHcv8K0\nnwN+Hbg8kVAjGiH73wU+VVXPAlTVusk/QvYCvjtJgNcD3wKuTCheX3uBj9XAI8Abk2yedqhRVdUX\nq+oPuruPMHjfz7VkXT5fR7Cmz1dLYHQfAj4AfHuplUm2AH+HwV9z682K2YEfAK5P8oUkjyV59+Si\nrWq17P8W+MvA7wGPA++vquXmTloB/7H7mR5YYv1SH6OyZSLJRrNa/mF3Aw9NINOoVsy+zp+vq/3c\n1/T56umgESR5O3C5qh5Lcusy0z4E/EJVfXvwR+n6MGL2jcCbgduA1wC/neSRqvqdCcVc0ojZbwe+\nCrwV+H7gVJL/WlUvTijmSn6yqi51h+unknyjqh6edqirMFL+JD/FoAR+cuIJl7da9nX5fO2sln1N\nn68eCYzmLcA7kjzN4JNP35rkVxfNmQWOd3PeCRxJcudEUy5tlOwXgd+qqj/szkM+DKyHi3yjZH8P\ng0Pjqqp5Buepf2iyMZdWVZe628vApxl8iu6wkT5GZVpGyE+SH2Zwqm5vVf3+ZBMub4Ts6/X5Okr2\ntX2+VpVfV/EF3Ap8ZpU5DwDvnHbWUbMzOJ1ymsFfGK8FngBunnbeEbPfB/yzbnkTg1+iN66DvK8D\nvnto+YsMLuQNz7mDwSmUALcAj04791XmfxMwD/zEtPNebfZF89fN83XEn/uaPl89HdRDkp8FqKp/\nP+0sV2s4e1U9meRzwNcZnHu/v6qemGrAFSz6uf9z4IEkjzP4ZfoLtcIrQiZoE/Dp7lTDRuDXqupz\ni7J/FvhbDH6R/hGDo5r1YpT8vwT8RQZ/RQNcqfXx4WyjZF+vVs2+1s9X3zEsSQ3zmoAkNcwSkKSG\nWQKS1DBLQJIaZglIUsMsAUlqmCUgSQ2zBCSpYf8P/iVY5tIxAvQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xb0a95c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "plt.hist(log_train_y)\n",
    "train_y = log_train_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# do some feature engineering\n",
    "generate a new feature according to https://www.kaggle.com/headsortails/mercedas-update2-intrinsic-noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#X10-X395\n",
    "train_data = pd.concat([train_X,train_y],axis=1)\n",
    "train_data = train_data.sort_values( by= 'y')\n",
    "train_data['diff'] = 0\n",
    "ref_row = train_data.iloc[0]\n",
    "predictors = [item for item in train_data.columns if item !='y']\n",
    "for ind,row in train_data.iterrows():\n",
    "    diff = 0.0\n",
    "    for ind_feat in predictors:\n",
    "        feat = ind_feat\n",
    "        diff += np.abs(ref_row[feat]-row[feat])\n",
    "    #row['diff'] = diff\n",
    "    train_data.set_value(ind,'diff',diff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_X['diff'] = 0\n",
    "for ind,row in test_X.iterrows():\n",
    "    diff = 0.0\n",
    "    for ind_feat in predictors:\n",
    "        diff += np.abs(ref_row[ind_feat]-row[ind_feat])\n",
    "    test_X.set_value(ind,'diff',diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# normalization for the 'diff' feature\n",
    "train_data['diff'] = (train_data['diff']-train_data['diff'].min())/(train_data['diff'].max()-train_data['diff'].min())\n",
    "test_X['diff'] = (test_X['diff']-test_X['diff'].min())/(test_X['diff'].max()-test_X['diff'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "train_X = train_data[predictors]\n",
    "train_y = train_data['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# use elastic net for modelling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's perform a cross-validation to find the best combination of alpha and l1_ratio\n",
    "from sklearn.linear_model import ElasticNetCV, ElasticNet\n",
    "\n",
    "cv_model = ElasticNetCV(l1_ratio=[.1, .5, .7, .9, .95, .99, .995, 1], eps=0.001, n_alphas=100, fit_intercept=True, \n",
    "                        normalize=True, precompute='auto', max_iter=2000, tol=0.0001, cv=5, \n",
    "                        copy_X=True, verbose=0, n_jobs=-1, positive=False, random_state=None, selection='cyclic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElasticNetCV(alphas=None, copy_X=True, cv=5, eps=0.001, fit_intercept=True,\n",
       "       l1_ratio=[0.1, 0.5, 0.7, 0.9, 0.95, 0.99, 0.995, 1], max_iter=2000,\n",
       "       n_alphas=100, n_jobs=-1, normalize=True, positive=False,\n",
       "       precompute='auto', random_state=None, selection='cyclic',\n",
       "       tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_model.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal alpha: 0.00314540\n",
      "Optimal l1_ratio: 1.000\n",
      "Number of iterations 603\n"
     ]
    }
   ],
   "source": [
    "print('Optimal alpha: %.8f'%cv_model.alpha_)\n",
    "print('Optimal l1_ratio: %.3f'%cv_model.l1_ratio_)\n",
    "print('Number of iterations %d'%cv_model.n_iter_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElasticNet(alpha=0.003145399303881424, copy_X=True, fit_intercept=True,\n",
       "      l1_ratio=1.0, max_iter=603, normalize=True, positive=False,\n",
       "      precompute=False, random_state=None, selection='cyclic', tol=0.0001,\n",
       "      warm_start=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train model with best parameters from CV\n",
    "model = ElasticNet(l1_ratio=cv_model.l1_ratio_, alpha = cv_model.alpha_, max_iter=cv_model.n_iter_, fit_intercept=True, normalize = True)\n",
    "model.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.586978770942\n"
     ]
    }
   ],
   "source": [
    "# r2 score on training dataset\n",
    "print(r2_score(train_y, model.predict(train_X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = model.predict(test_X)\n",
    "df_sub = pd.DataFrame({'ID': test_ids, 'y': preds})\n",
    "df_sub.to_csv('elnet_submission_dummies.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 features, reduction of 100.00%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0xe672240>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABBIAAAF9CAYAAAC5wkvuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xe672390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_importance = pd.Series(index = train_X.columns, data = np.abs(model.coef_))\n",
    "\n",
    "n_selected_features = (feature_importance>0).sum()\n",
    "print('{0:d} features, reduction of {1:2.2f}%'.format(\n",
    "    n_selected_features,(1-n_selected_features/len(feature_importance))*100))\n",
    "\n",
    "feature_importance.sort_values().tail(30).plot(kind = 'bar', figsize = (18,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# use xgboost modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def R2score(pred,dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    return 'score',metrics.r2_score(labels,pred)\n",
    "    \n",
    "def modelfit(alg, dtrain, ytrain , predictors,test, useTrainCV=True, cv_folds=10, early_stopping_rounds=50):\n",
    "    \n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(dtrain, label=ytrain)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "                          feval=R2score,maximize = True, early_stopping_rounds=early_stopping_rounds)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "        print(cvresult)\n",
    "        print('num_rounds %f'%cvresult.shape[0])\n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(dtrain, ytrain,eval_metric='rmse')\n",
    "        \n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(dtrain)\n",
    "        \n",
    "    #Print model report:\n",
    "    print (\"\\nModel Report\")\n",
    "    print (\"R2 : %.4g\" % metrics.r2_score(ytrain, dtrain_predictions))\n",
    "\n",
    "    #feat_imp = pd.Series(alg.booster().get_fscore()).sort_values(ascending=False)\n",
    "    #feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "    #plt.ylabel('Feature Importance Score')\n",
    "    \n",
    "  \n",
    "    #dtest = xgb.DMatrix(test[predictors].values)\n",
    "    #y_pred = alg.predict(test[predictors].values)\n",
    "    \n",
    "    #output = pd.DataFrame({'id': test['ID'].astype(np.int32), 'y': y_pred})\n",
    "    #output.to_csv('submission_baseLine.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    test-rmse-mean  test-rmse-std  test-score-mean  test-score-std  \\\n",
      "0         3.704894       0.006779      -953.403407       70.112572   \n",
      "1         3.334949       0.006652      -772.320300       56.841897   \n",
      "2         3.001877       0.006496      -625.560103       45.988506   \n",
      "3         2.702176       0.006475      -506.695760       37.270657   \n",
      "4         2.432497       0.006368      -410.414932       30.206959   \n",
      "5         2.189717       0.006240      -332.390063       24.501052   \n",
      "6         1.971291       0.006071      -269.193732       19.839446   \n",
      "7         1.774682       0.005836      -217.981671       16.037158   \n",
      "8         1.597728       0.005722      -176.487473       12.980142   \n",
      "9         1.438524       0.005607      -142.877370       10.512367   \n",
      "10        1.295288       0.005454      -115.648803        8.490354   \n",
      "11        1.166481       0.005447       -93.601451        6.879439   \n",
      "12        1.050450       0.005324       -75.716496        5.575571   \n",
      "13        0.946117       0.005230       -61.233100        4.516600   \n",
      "14        0.852293       0.005168       -49.501026        3.658505   \n",
      "15        0.767878       0.005186       -39.991741        2.962273   \n",
      "16        0.691940       0.005189       -32.284409        2.404762   \n",
      "17        0.623700       0.005195       -26.042805        1.957664   \n",
      "18        0.562361       0.005268       -20.984584        1.590392   \n",
      "19        0.507189       0.005278       -16.881693        1.289657   \n",
      "20        0.457678       0.005347       -13.560266        1.049033   \n",
      "21        0.413231       0.005422       -10.868976        0.854062   \n",
      "22        0.373351       0.005507        -8.688196        0.698862   \n",
      "23        0.337627       0.005609        -6.922655        0.576038   \n",
      "24        0.305661       0.005682        -5.493212        0.475975   \n",
      "25        0.277016       0.005782        -4.332970        0.394789   \n",
      "26        0.251466       0.005915        -3.394460        0.331685   \n",
      "27        0.228590       0.006066        -2.631258        0.281537   \n",
      "28        0.208263       0.006249        -2.014002        0.239618   \n",
      "29        0.190209       0.006421        -1.514068        0.207556   \n",
      "..             ...            ...              ...             ...   \n",
      "42        0.085705       0.008033         0.488213        0.085405   \n",
      "43        0.083455       0.008012         0.514633        0.082942   \n",
      "44        0.081581       0.007998         0.536125        0.080780   \n",
      "45        0.080048       0.007966         0.553348        0.078853   \n",
      "46        0.078781       0.007965         0.567333        0.077509   \n",
      "47        0.077744       0.007960         0.578591        0.076486   \n",
      "48        0.076917       0.007921         0.587491        0.075339   \n",
      "49        0.076188       0.007885         0.595292        0.074137   \n",
      "50        0.075620       0.007842         0.601340        0.072940   \n",
      "51        0.075156       0.007788         0.606256        0.071859   \n",
      "52        0.074752       0.007769         0.610491        0.071197   \n",
      "53        0.074434       0.007729         0.613805        0.070482   \n",
      "54        0.074170       0.007677         0.616554        0.069746   \n",
      "55        0.073947       0.007628         0.618909        0.068879   \n",
      "56        0.073803       0.007552         0.620445        0.067943   \n",
      "57        0.073693       0.007518         0.621626        0.067307   \n",
      "58        0.073588       0.007471         0.622760        0.066518   \n",
      "59        0.073528       0.007415         0.623425        0.065862   \n",
      "60        0.073457       0.007280         0.624224        0.064332   \n",
      "61        0.073398       0.007245         0.624842        0.063967   \n",
      "62        0.073383       0.007215         0.625013        0.063647   \n",
      "63        0.073322       0.007222         0.625614        0.063739   \n",
      "64        0.073303       0.007203         0.625855        0.063372   \n",
      "65        0.073281       0.007171         0.626107        0.062958   \n",
      "66        0.073262       0.007151         0.626311        0.062770   \n",
      "67        0.073275       0.007132         0.626191        0.062503   \n",
      "68        0.073251       0.007117         0.626446        0.062344   \n",
      "69        0.073216       0.007126         0.626807        0.062338   \n",
      "70        0.073215       0.007116         0.626831        0.062214   \n",
      "71        0.073192       0.007096         0.627072        0.061980   \n",
      "\n",
      "    train-rmse-mean  train-rmse-std  train-score-mean  train-score-std  \n",
      "0          3.704814        0.000653       -945.466282         7.677031  \n",
      "1          3.334848        0.000583       -765.874787         6.209106  \n",
      "2          3.001861        0.000558       -620.374706         5.046033  \n",
      "3          2.702178        0.000475       -502.501043         4.098551  \n",
      "4          2.432464        0.000446       -407.004890         3.319884  \n",
      "5          2.189732        0.000386       -329.639338         2.690603  \n",
      "6          1.971260        0.000373       -266.954047         2.173462  \n",
      "7          1.774695        0.000356       -216.179937         1.754532  \n",
      "8          1.597769        0.000314       -175.035440         1.422546  \n",
      "9          1.438582        0.000288       -141.705727         1.156729  \n",
      "10         1.295329        0.000305       -114.699740         0.934695  \n",
      "11         1.166443        0.000257        -92.820838         0.759809  \n",
      "12         1.050432        0.000246        -75.086529         0.612168  \n",
      "13         0.946094        0.000236        -60.722154         0.499098  \n",
      "14         0.852256        0.000226        -49.085565         0.404189  \n",
      "15         0.767844        0.000199        -39.655344         0.326693  \n",
      "16         0.691928        0.000178        -32.013659         0.263246  \n",
      "17         0.623678        0.000222        -25.822066         0.208938  \n",
      "18         0.562344        0.000215        -20.805966         0.168799  \n",
      "19         0.507206        0.000246        -16.739449         0.137282  \n",
      "20         0.457699        0.000251        -13.445441         0.112117  \n",
      "21         0.413289        0.000262        -10.778201         0.092491  \n",
      "22         0.373411        0.000269         -8.614917         0.076113  \n",
      "23         0.337666        0.000267         -6.862257         0.062173  \n",
      "24         0.305678        0.000283         -5.443171         0.050289  \n",
      "25         0.277025        0.000283         -4.291868         0.041209  \n",
      "26         0.251454        0.000288         -3.359997         0.033194  \n",
      "27         0.228565        0.000313         -2.602361         0.026332  \n",
      "28         0.208210        0.000309         -1.989315         0.021916  \n",
      "29         0.190100        0.000326         -1.491917         0.017504  \n",
      "..              ...             ...               ...              ...  \n",
      "42         0.083991        0.000682          0.513545         0.006901  \n",
      "43         0.081550        0.000693          0.541410         0.006773  \n",
      "44         0.079498        0.000703          0.564191         0.006833  \n",
      "45         0.077735        0.000686          0.583311         0.006511  \n",
      "46         0.076274        0.000695          0.598825         0.006391  \n",
      "47         0.075031        0.000734          0.611792         0.006710  \n",
      "48         0.073932        0.000751          0.623083         0.006851  \n",
      "49         0.073032        0.000745          0.632192         0.006859  \n",
      "50         0.072278        0.000754          0.639755         0.006718  \n",
      "51         0.071627        0.000747          0.646211         0.006466  \n",
      "52         0.071079        0.000793          0.651602         0.006891  \n",
      "53         0.070587        0.000810          0.656409         0.007172  \n",
      "54         0.070124        0.000859          0.660895         0.007661  \n",
      "55         0.069731        0.000817          0.664682         0.007203  \n",
      "56         0.069369        0.000843          0.668156         0.007484  \n",
      "57         0.069055        0.000849          0.671155         0.007474  \n",
      "58         0.068747        0.000838          0.674080         0.007398  \n",
      "59         0.068482        0.000809          0.676588         0.007145  \n",
      "60         0.068262        0.000825          0.678662         0.007164  \n",
      "61         0.068041        0.000809          0.680743         0.006956  \n",
      "62         0.067839        0.000787          0.682634         0.006631  \n",
      "63         0.067620        0.000789          0.684686         0.006645  \n",
      "64         0.067440        0.000779          0.686367         0.006546  \n",
      "65         0.067279        0.000795          0.687853         0.006682  \n",
      "66         0.067122        0.000776          0.689312         0.006646  \n",
      "67         0.066951        0.000780          0.690893         0.006680  \n",
      "68         0.066831        0.000765          0.691997         0.006486  \n",
      "69         0.066656        0.000765          0.693611         0.006450  \n",
      "70         0.066547        0.000771          0.694607         0.006513  \n",
      "71         0.066414        0.000774          0.695829         0.006608  \n",
      "\n",
      "[72 rows x 8 columns]\n",
      "num_rounds 72.000000\n",
      "\n",
      "Model Report\n",
      "R2 : 0.6911\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#Choose all predictors except target & IDcols\n",
    "test_X['ID'] = test_ids\n",
    "predictors = test_X.dtypes.index[test_X.columns!='ID']\n",
    "xgb1 = XGBRegressor(\n",
    "                     learning_rate =0.1,\n",
    "                     n_estimators=1000,\n",
    "                     max_depth=5,\n",
    "                     min_child_weight=1,\n",
    "                     gamma=0,\n",
    "                     subsample=0.8,\n",
    "                     colsample_bytree=0.8,\n",
    "                     objective= 'reg:linear',\n",
    "                     #base_score = y_mean,\n",
    "                     nthread=4,\n",
    "                     scale_pos_weight=1,\n",
    "                     seed=27)\n",
    "                     \n",
    "modelfit(xgb1,train_X.values, train_y, predictors,test_X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try the magic feature\n",
    "https://www.kaggle.com/robertoruiz/a-magic-feature/code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID  X0 X1  X2 X3 X4 X5 X6 X8  X10  ...   X375  X376  X377  X378  X379  \\\n",
      "0   0   k  v  at  a  d  u  j  o    0  ...      0     0     1     0     0   \n",
      "1   6   k  t  av  e  d  y  l  o    0  ...      1     0     0     0     0   \n",
      "2   7  az  w   n  c  d  x  j  x    0  ...      0     0     0     0     0   \n",
      "3   9  az  t   n  f  d  x  l  e    0  ...      0     0     0     0     0   \n",
      "4  13  az  v   n  f  d  h  d  n    0  ...      0     0     0     0     0   \n",
      "\n",
      "   X380  X382  X383  X384  X385  \n",
      "0     0     0     0     0     0  \n",
      "1     0     0     0     0     0  \n",
      "2     0     1     0     0     0  \n",
      "3     0     0     0     0     0  \n",
      "4     0     0     0     0     0  \n",
      "\n",
      "[5 rows x 377 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train_df.head())\n",
    "train_df['y'] = train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x0_y_mean = train_df.groupby(by = 'X0')['y'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def assign_x0_mean(row):\n",
    "    if row['X0'] in x0_y_mean:\n",
    "        row['x0_y_mean'] = x0_y_mean[row['X0']]\n",
    "    else:\n",
    "        row['x0_y_mean'] = None\n",
    "    return row['x0_y_mean']\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train_df['x0_y_mean'] = -1\n",
    "train_df['x0_y_mean'] = train_df.apply(assign_x0_mean,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_df['x0_y_mean'] = test_df.apply(assign_x0_mean,axis=1)\n",
    "test_df['x0_y_mean'] = test_df['x0_y_mean'].fillna(test_df['x0_y_mean'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# concatenate the features with train_X\n",
    "train_X['x0_y_mean'] = train_df['x0_y_mean'].values\n",
    "test_X['x0_y_mean'] = test_df['x0_y_mean'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "R2 : 0.634\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEVCAYAAAACW4lMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xc4e51d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Choose all predictors except target & IDcols\n",
    "test_X['ID'] = test_ids\n",
    "predictors = test_X.dtypes.index[test_X.columns!='ID']\n",
    "xgb1 = XGBRegressor(\n",
    "                     learning_rate =0.1,\n",
    "                     n_estimators=1000,\n",
    "                     max_depth=5,\n",
    "                     min_child_weight=1,\n",
    "                     gamma=0,\n",
    "                     subsample=0.8,\n",
    "                     colsample_bytree=0.8,\n",
    "                     objective= 'reg:linear',\n",
    "                     #base_score = y_mean,\n",
    "                     nthread=4,\n",
    "                     scale_pos_weight=1,\n",
    "                     seed=27)\n",
    "                     \n",
    "modelfit(xgb1,train_X.values, train_y, predictors,test_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#upload the feature directly w\n",
    "ret_df = test_X[['ID','x0_y_mean']]\n",
    "ret_df.columns=['id','y']\n",
    "ret_df.to_csv('test.csv',index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tuning the parameter for xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    test-rmse-mean  test-rmse-std  test-score-mean  test-score-std  \\\n",
      "0         3.704793       0.006541      -953.348681       70.073531   \n",
      "1         3.334759       0.006440      -772.228560       56.787973   \n",
      "2         3.001736       0.006368      -625.503876       46.024803   \n",
      "3         2.702028       0.006322      -506.642790       37.305061   \n",
      "4         2.432281       0.006054      -410.342282       30.204074   \n",
      "5         2.189516       0.005828      -332.325931       24.454040   \n",
      "6         1.971039       0.005640      -269.122202       19.798158   \n",
      "7         1.774428       0.005483      -217.918702       16.028442   \n",
      "8         1.597501       0.005355      -176.437004       12.976398   \n",
      "9         1.438266       0.005232      -142.825630       10.506010   \n",
      "10        1.295000       0.005135      -115.598172        8.503859   \n",
      "11        1.166086       0.005094       -93.537989        6.883040   \n",
      "12        1.050095       0.005061       -75.664571        5.571442   \n",
      "13        0.945754       0.005045       -61.185060        4.511047   \n",
      "14        0.851920       0.005060       -49.456893        3.657280   \n",
      "15        0.767530       0.005070       -39.955057        2.967563   \n",
      "16        0.691623       0.005120       -32.254291        2.408881   \n",
      "17        0.623349       0.005119       -26.011975        1.950329   \n",
      "18        0.562028       0.005211       -20.958199        1.584320   \n",
      "19        0.506923       0.005267       -16.862871        1.288362   \n",
      "20        0.457426       0.005330       -13.544347        1.049348   \n",
      "21        0.413015       0.005376       -10.856895        0.857437   \n",
      "22        0.373153       0.005448        -8.678048        0.699687   \n",
      "23        0.337457       0.005543        -6.914783        0.576606   \n",
      "24        0.305429       0.005638        -5.483456        0.476456   \n",
      "25        0.276780       0.005778        -4.323963        0.395753   \n",
      "26        0.251188       0.005893        -3.384754        0.330900   \n",
      "27        0.228365       0.006046        -2.624058        0.280510   \n",
      "28        0.208075       0.006171        -2.008602        0.239133   \n",
      "29        0.189995       0.006340        -1.508438        0.206781   \n",
      "..             ...            ...              ...             ...   \n",
      "41        0.088369       0.007859         0.456113        0.086458   \n",
      "42        0.085647       0.007883         0.488986        0.083837   \n",
      "43        0.083425       0.007908         0.515047        0.081832   \n",
      "44        0.081549       0.007916         0.536537        0.079965   \n",
      "45        0.080010       0.007895         0.553832        0.078094   \n",
      "46        0.078745       0.007859         0.567803        0.076390   \n",
      "47        0.077681       0.007840         0.579380        0.075082   \n",
      "48        0.076827       0.007795         0.588590        0.073680   \n",
      "49        0.076113       0.007760         0.596199        0.072563   \n",
      "50        0.075552       0.007711         0.602135        0.071514   \n",
      "51        0.075091       0.007674         0.606971        0.070729   \n",
      "52        0.074728       0.007643         0.610790        0.070014   \n",
      "53        0.074414       0.007607         0.614062        0.069306   \n",
      "54        0.074168       0.007584         0.616632        0.068796   \n",
      "55        0.073962       0.007539         0.618779        0.068150   \n",
      "56        0.073823       0.007475         0.620259        0.067340   \n",
      "57        0.073698       0.007445         0.621568        0.066862   \n",
      "58        0.073591       0.007411         0.622697        0.066344   \n",
      "59        0.073490       0.007381         0.623754        0.065916   \n",
      "60        0.073423       0.007356         0.624454        0.065601   \n",
      "61        0.073376       0.007335         0.624957        0.065318   \n",
      "62        0.073341       0.007313         0.625311        0.065133   \n",
      "63        0.073302       0.007307         0.625719        0.065019   \n",
      "64        0.073276       0.007285         0.625998        0.064773   \n",
      "65        0.073262       0.007259         0.626159        0.064465   \n",
      "66        0.073236       0.007250         0.626439        0.064292   \n",
      "67        0.073229       0.007238         0.626522        0.064147   \n",
      "68        0.073219       0.007228         0.626633        0.064062   \n",
      "69        0.073207       0.007213         0.626765        0.063927   \n",
      "70        0.073201       0.007209         0.626829        0.063844   \n",
      "\n",
      "    train-rmse-mean  train-rmse-std  train-score-mean  train-score-std  \n",
      "0          3.704797        0.000669       -945.457919         7.679207  \n",
      "1          3.334762        0.000603       -765.835343         6.220730  \n",
      "2          3.001738        0.000544       -620.323864         5.039203  \n",
      "3          2.702030        0.000491       -502.445635         4.082061  \n",
      "4          2.432280        0.000444       -406.942965         3.306754  \n",
      "5          2.189513        0.000401       -329.572855         2.678653  \n",
      "6          1.971034        0.000363       -266.892565         2.169806  \n",
      "7          1.774422        0.000329       -216.113026         1.757563  \n",
      "8          1.597493        0.000299       -174.974709         1.423598  \n",
      "9          1.438275        0.000271       -141.644945         1.152937  \n",
      "10         1.295004        0.000250       -114.641730         0.933828  \n",
      "11         1.166089        0.000230        -92.763910         0.755829  \n",
      "12         1.050103        0.000213        -75.038900         0.612280  \n",
      "13         0.945761        0.000198        -60.678704         0.495780  \n",
      "14         0.851905        0.000187        -49.044302         0.401488  \n",
      "15         0.767494        0.000179        -39.618374         0.325061  \n",
      "16         0.691592        0.000173        -31.981556         0.262921  \n",
      "17         0.623352        0.000170        -25.794072         0.212721  \n",
      "18         0.562016        0.000170        -20.780529         0.172078  \n",
      "19         0.506909        0.000173        -16.718656         0.139115  \n",
      "20         0.457416        0.000179        -13.427560         0.112427  \n",
      "21         0.412989        0.000186        -10.761074         0.090843  \n",
      "22         0.373129        0.000197         -8.600393         0.073333  \n",
      "23         0.337395        0.000209         -6.849604         0.059136  \n",
      "24         0.305383        0.000225         -5.430717         0.047749  \n",
      "25         0.276739        0.000243         -4.280940         0.038569  \n",
      "26         0.251139        0.000263         -3.349094         0.031211  \n",
      "27         0.228293        0.000286         -2.593794         0.025382  \n",
      "28         0.207939        0.000310         -1.981527         0.020776  \n",
      "29         0.189832        0.000334         -1.484887         0.017213  \n",
      "..              ...             ...               ...              ...  \n",
      "41         0.086657        0.000620          0.482182         0.006513  \n",
      "42         0.083805        0.000640          0.515710         0.006439  \n",
      "43         0.081347        0.000657          0.543698         0.006420  \n",
      "44         0.079316        0.000667          0.566193         0.006306  \n",
      "45         0.077579        0.000695          0.584981         0.006276  \n",
      "46         0.076136        0.000692          0.600280         0.005976  \n",
      "47         0.074930        0.000693          0.612854         0.005844  \n",
      "48         0.073921        0.000697          0.623201         0.005797  \n",
      "49         0.073064        0.000704          0.631891         0.005673  \n",
      "50         0.072364        0.000700          0.638906         0.005741  \n",
      "51         0.071749        0.000701          0.645016         0.005667  \n",
      "52         0.071213        0.000686          0.650305         0.005451  \n",
      "53         0.070800        0.000682          0.654352         0.005359  \n",
      "54         0.070452        0.000719          0.657736         0.005624  \n",
      "55         0.070119        0.000718          0.660970         0.005585  \n",
      "56         0.069840        0.000697          0.663655         0.005455  \n",
      "57         0.069626        0.000696          0.665719         0.005432  \n",
      "58         0.069443        0.000688          0.667468         0.005280  \n",
      "59         0.069224        0.000674          0.669563         0.005210  \n",
      "60         0.069065        0.000668          0.671081         0.005308  \n",
      "61         0.068929        0.000673          0.672373         0.005415  \n",
      "62         0.068754        0.000691          0.674033         0.005679  \n",
      "63         0.068657        0.000695          0.674950         0.005712  \n",
      "64         0.068528        0.000685          0.676163         0.005773  \n",
      "65         0.068411        0.000650          0.677272         0.005417  \n",
      "66         0.068299        0.000666          0.678333         0.005448  \n",
      "67         0.068183        0.000660          0.679427         0.005378  \n",
      "68         0.068061        0.000650          0.680573         0.005181  \n",
      "69         0.067960        0.000646          0.681517         0.005255  \n",
      "70         0.067832        0.000624          0.682719         0.005105  \n",
      "\n",
      "[71 rows x 8 columns]\n",
      "num_rounds 71.000000\n",
      "\n",
      "Model Report\n",
      "R2 : 0.6783\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Choose all predictors except target & IDcols\n",
    "test_X['ID'] = test_ids\n",
    "predictors = test_X.dtypes.index[test_X.columns!='ID']\n",
    "xgb1 = XGBRegressor(\n",
    "                     learning_rate =0.1,\n",
    "                     n_estimators=1000,\n",
    "                     max_depth=5,\n",
    "                     min_child_weight=1,\n",
    "                     gamma=0,\n",
    "                     #subsample=0.8,\n",
    "                     #colsample_bytree=0.8,\n",
    "                     objective= 'reg:linear',\n",
    "                     #base_score = y_mean,\n",
    "                     nthread=4,\n",
    "                     scale_pos_weight=1,\n",
    "                     seed=27)\n",
    "                     \n",
    "modelfit(xgb1,train_X.values, train_y, predictors,test_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\softwareinstallion\\anaconda2\\lib\\site-packages\\sklearn\\model_selection\\_search.py:667: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([mean: -0.00549, std: 0.00114, params: {'max_depth': 2},\n",
       "  mean: -0.00546, std: 0.00114, params: {'max_depth': 3},\n",
       "  mean: -0.00547, std: 0.00114, params: {'max_depth': 4},\n",
       "  mean: -0.00550, std: 0.00115, params: {'max_depth': 5},\n",
       "  mean: -0.00552, std: 0.00114, params: {'max_depth': 6}],\n",
       " {'max_depth': 3},\n",
       " -0.0054631687439152055)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#continue increase the power of the model \n",
    "param_test = {\n",
    " 'max_depth':[2,3,4,5,6]\n",
    "}\n",
    "gsearch = GridSearchCV(estimator = XGBRegressor( \n",
    "    learning_rate =0.1,\n",
    "    n_estimators=71,\n",
    "    max_depth=5,\n",
    "    min_child_weight=1,\n",
    "    gamma=0,\n",
    "    objective= 'reg:linear',\n",
    "    nthread=3,\n",
    "    scale_pos_weight=1,\n",
    "    seed=27), \n",
    "param_grid = param_test, scoring='neg_mean_squared_error',n_jobs=4,iid=False, cv=5)\n",
    "gsearch.fit(train_X.values,train_y)\n",
    "gsearch.grid_scores_, gsearch.best_params_, gsearch.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    test-rmse-mean  test-rmse-std  test-score-mean  test-score-std  \\\n",
      "0         3.704793       0.006541      -953.348681       70.073531   \n",
      "1         3.334759       0.006440      -772.228560       56.787973   \n",
      "2         3.001736       0.006368      -625.503876       46.024803   \n",
      "3         2.702028       0.006322      -506.642790       37.305061   \n",
      "4         2.432281       0.006054      -410.342282       30.204074   \n",
      "5         2.189516       0.005828      -332.325931       24.454040   \n",
      "6         1.971039       0.005640      -269.122202       19.798158   \n",
      "7         1.774428       0.005483      -217.918702       16.028442   \n",
      "8         1.597501       0.005355      -176.437004       12.976398   \n",
      "9         1.438266       0.005232      -142.825630       10.506010   \n",
      "10        1.295000       0.005135      -115.598172        8.503859   \n",
      "11        1.166086       0.005094       -93.537989        6.883040   \n",
      "12        1.050095       0.005061       -75.664571        5.571442   \n",
      "13        0.945754       0.005045       -61.185060        4.511047   \n",
      "14        0.851920       0.005060       -49.456893        3.657280   \n",
      "15        0.767530       0.005070       -39.955057        2.967563   \n",
      "16        0.691623       0.005120       -32.254291        2.408881   \n",
      "17        0.623349       0.005119       -26.011975        1.950329   \n",
      "18        0.562029       0.005212       -20.958257        1.584270   \n",
      "19        0.506920       0.005240       -16.862724        1.288591   \n",
      "20        0.457434       0.005305       -13.544987        1.050892   \n",
      "21        0.413011       0.005365       -10.856432        0.853781   \n",
      "22        0.373158       0.005444        -8.678404        0.700423   \n",
      "23        0.337447       0.005559        -6.914261        0.576030   \n",
      "24        0.305424       0.005656        -5.483036        0.473981   \n",
      "25        0.276790       0.005757        -4.324265        0.394565   \n",
      "26        0.251217       0.005876        -3.385702        0.329986   \n",
      "27        0.228391       0.005970        -2.624776        0.277928   \n",
      "28        0.208038       0.006148        -2.007404        0.237156   \n",
      "29        0.189997       0.006306        -1.508371        0.204718   \n",
      "..             ...            ...              ...             ...   \n",
      "51        0.074930       0.007699         0.608669        0.070660   \n",
      "52        0.074546       0.007656         0.612696        0.069795   \n",
      "53        0.074247       0.007633         0.615812        0.069229   \n",
      "54        0.073992       0.007607         0.618456        0.068696   \n",
      "55        0.073812       0.007570         0.620332        0.068156   \n",
      "56        0.073641       0.007536         0.622110        0.067610   \n",
      "57        0.073475       0.007527         0.623819        0.067328   \n",
      "58        0.073377       0.007502         0.624826        0.067012   \n",
      "59        0.073312       0.007471         0.625523        0.066583   \n",
      "60        0.073244       0.007438         0.626242        0.066158   \n",
      "61        0.073187       0.007409         0.626837        0.065841   \n",
      "62        0.073138       0.007392         0.627345        0.065630   \n",
      "63        0.073098       0.007378         0.627770        0.065363   \n",
      "64        0.073061       0.007387         0.628145        0.065402   \n",
      "65        0.073042       0.007365         0.628344        0.065162   \n",
      "66        0.072999       0.007361         0.628790        0.065091   \n",
      "67        0.073002       0.007334         0.628785        0.064759   \n",
      "68        0.072996       0.007320         0.628865        0.064583   \n",
      "69        0.072996       0.007312         0.628870        0.064446   \n",
      "70        0.072988       0.007303         0.628969        0.064313   \n",
      "71        0.072986       0.007288         0.629005        0.064135   \n",
      "72        0.073003       0.007288         0.628830        0.064135   \n",
      "73        0.073008       0.007274         0.628794        0.063975   \n",
      "74        0.072998       0.007279         0.628893        0.064001   \n",
      "75        0.072989       0.007275         0.628990        0.063958   \n",
      "76        0.072985       0.007280         0.629039        0.063971   \n",
      "77        0.072986       0.007278         0.629037        0.063872   \n",
      "78        0.072983       0.007276         0.629078        0.063789   \n",
      "79        0.072982       0.007269         0.629103        0.063715   \n",
      "80        0.072980       0.007269         0.629118        0.063729   \n",
      "\n",
      "    train-rmse-mean  train-rmse-std  train-score-mean  train-score-std  \n",
      "0          3.704797        0.000669       -945.457919         7.679207  \n",
      "1          3.334762        0.000603       -765.835343         6.220730  \n",
      "2          3.001738        0.000544       -620.323864         5.039203  \n",
      "3          2.702030        0.000491       -502.445635         4.082061  \n",
      "4          2.432280        0.000444       -406.942965         3.306754  \n",
      "5          2.189513        0.000401       -329.572855         2.678653  \n",
      "6          1.971034        0.000363       -266.892565         2.169806  \n",
      "7          1.774422        0.000329       -216.113026         1.757563  \n",
      "8          1.597493        0.000299       -174.974709         1.423598  \n",
      "9          1.438275        0.000271       -141.644945         1.152937  \n",
      "10         1.295004        0.000250       -114.641730         0.933828  \n",
      "11         1.166089        0.000230        -92.763910         0.755829  \n",
      "12         1.050103        0.000213        -75.038900         0.612280  \n",
      "13         0.945761        0.000198        -60.678704         0.495780  \n",
      "14         0.851905        0.000187        -49.044302         0.401488  \n",
      "15         0.767494        0.000179        -39.618374         0.325061  \n",
      "16         0.691592        0.000173        -31.981556         0.262921  \n",
      "17         0.623352        0.000170        -25.794072         0.212721  \n",
      "18         0.562016        0.000169        -20.780563         0.172111  \n",
      "19         0.506913        0.000173        -16.718930         0.139095  \n",
      "20         0.457420        0.000179        -13.427823         0.112356  \n",
      "21         0.412994        0.000188        -10.761400         0.090722  \n",
      "22         0.373136        0.000198         -8.600742         0.073264  \n",
      "23         0.337405        0.000212         -6.850070         0.059073  \n",
      "24         0.305399        0.000227         -5.431406         0.047656  \n",
      "25         0.276760        0.000243         -4.281731         0.038485  \n",
      "26         0.251173        0.000263         -3.350252         0.031033  \n",
      "27         0.228339        0.000283         -2.595249         0.025092  \n",
      "28         0.208004        0.000307         -1.983386         0.020344  \n",
      "29         0.189930        0.000338         -1.487453         0.016544  \n",
      "..              ...             ...               ...              ...  \n",
      "51         0.073903        0.000870          0.623375         0.007489  \n",
      "52         0.073449        0.000874          0.627988         0.007486  \n",
      "53         0.073083        0.000888          0.631691         0.007577  \n",
      "54         0.072770        0.000893          0.634834         0.007609  \n",
      "55         0.072496        0.000892          0.637573         0.007613  \n",
      "56         0.072274        0.000904          0.639788         0.007696  \n",
      "57         0.072080        0.000905          0.641722         0.007662  \n",
      "58         0.071907        0.000900          0.643445         0.007654  \n",
      "59         0.071773        0.000895          0.644764         0.007610  \n",
      "60         0.071651        0.000893          0.645974         0.007562  \n",
      "61         0.071550        0.000892          0.646973         0.007542  \n",
      "62         0.071460        0.000900          0.647860         0.007592  \n",
      "63         0.071376        0.000893          0.648691         0.007509  \n",
      "64         0.071288        0.000894          0.649548         0.007484  \n",
      "65         0.071202        0.000905          0.650393         0.007719  \n",
      "66         0.071134        0.000905          0.651065         0.007702  \n",
      "67         0.071069        0.000895          0.651700         0.007600  \n",
      "68         0.070999        0.000903          0.652382         0.007779  \n",
      "69         0.070926        0.000883          0.653093         0.007665  \n",
      "70         0.070879        0.000880          0.653561         0.007633  \n",
      "71         0.070827        0.000874          0.654071         0.007573  \n",
      "72         0.070758        0.000882          0.654736         0.007584  \n",
      "73         0.070709        0.000888          0.655217         0.007666  \n",
      "74         0.070643        0.000899          0.655857         0.007841  \n",
      "75         0.070606        0.000895          0.656216         0.007825  \n",
      "76         0.070564        0.000901          0.656626         0.007931  \n",
      "77         0.070502        0.000896          0.657227         0.007797  \n",
      "78         0.070470        0.000895          0.657543         0.007772  \n",
      "79         0.070429        0.000883          0.657945         0.007643  \n",
      "80         0.070397        0.000881          0.658252         0.007591  \n",
      "\n",
      "[81 rows x 8 columns]\n",
      "num_rounds 81.000000\n",
      "\n",
      "Model Report\n",
      "R2 : 0.6581\n"
     ]
    }
   ],
   "source": [
    "xgb1 = XGBRegressor(\n",
    "                     learning_rate =0.1,\n",
    "                     n_estimators=500,\n",
    "                     max_depth=3,\n",
    "                     min_child_weight=1,\n",
    "                     gamma=0,\n",
    "                     #subsample=0.8,\n",
    "                     #colsample_bytree=0.8,\n",
    "                     objective= 'reg:linear',\n",
    "                     #base_score = y_mean,\n",
    "                     nthread=4,\n",
    "                     scale_pos_weight=1,\n",
    "                     seed=27)\n",
    "                     \n",
    "modelfit(xgb1,train_X.values, train_y, predictors,test_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "y_pred = xgb1.predict(test_X[predictors].values)\n",
    "#need to transform the y_pred to original scale\n",
    "y_pred = np.exp(y_pred)-1\n",
    "output = pd.DataFrame({'id': test_X['ID'].astype(np.int32), 'y': y_pred})\n",
    "output.to_csv('submission_baseLine_9.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\softwareinstallion\\anaconda2\\lib\\site-packages\\sklearn\\model_selection\\_search.py:667: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([mean: -0.00547, std: 0.00112, params: {'min_child_weight': 0.5},\n",
       "  mean: -0.00547, std: 0.00112, params: {'min_child_weight': 0.8},\n",
       "  mean: -0.00547, std: 0.00112, params: {'min_child_weight': 1},\n",
       "  mean: -0.00547, std: 0.00111, params: {'min_child_weight': 1.2},\n",
       "  mean: -0.00547, std: 0.00111, params: {'min_child_weight': 1.5}],\n",
       " {'min_child_weight': 0.5},\n",
       " -0.0054655396990520872)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#continue increase the power of the model \n",
    "param_test = {\n",
    " 'min_child_weight':[0.5,0.8,1,1.2,1.5]\n",
    "}\n",
    "gsearch = GridSearchCV(estimator = XGBRegressor( \n",
    "    learning_rate =0.1,\n",
    "    n_estimators=81,\n",
    "    max_depth=3,\n",
    "    #min_child_weight=1,\n",
    "    gamma=0,\n",
    "    objective= 'reg:linear',\n",
    "    nthread=3,\n",
    "    scale_pos_weight=1,\n",
    "    seed=27), \n",
    "param_grid = param_test, scoring='neg_mean_squared_error',n_jobs=4,iid=False, cv=5)\n",
    "gsearch.fit(train_X.values,train_y)\n",
    "gsearch.grid_scores_, gsearch.best_params_, gsearch.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    test-rmse-mean  test-rmse-std  test-score-mean  test-score-std  \\\n",
      "0         3.704793       0.006541      -953.348681       70.073531   \n",
      "1         3.334759       0.006440      -772.228560       56.787973   \n",
      "2         3.001736       0.006368      -625.503876       46.024803   \n",
      "3         2.702028       0.006322      -506.642790       37.305061   \n",
      "4         2.432281       0.006054      -410.342282       30.204074   \n",
      "5         2.189516       0.005828      -332.325931       24.454040   \n",
      "6         1.971039       0.005640      -269.122202       19.798158   \n",
      "7         1.774428       0.005483      -217.918702       16.028442   \n",
      "8         1.597501       0.005355      -176.437004       12.976398   \n",
      "9         1.438266       0.005232      -142.825630       10.506010   \n",
      "10        1.295000       0.005135      -115.598172        8.503859   \n",
      "11        1.166086       0.005094       -93.537989        6.883040   \n",
      "12        1.050095       0.005061       -75.664571        5.571442   \n",
      "13        0.945754       0.005045       -61.185060        4.511047   \n",
      "14        0.851920       0.005060       -49.456893        3.657280   \n",
      "15        0.767530       0.005070       -39.955057        2.967563   \n",
      "16        0.691623       0.005120       -32.254291        2.408881   \n",
      "17        0.623349       0.005119       -26.011975        1.950329   \n",
      "18        0.562029       0.005212       -20.958257        1.584270   \n",
      "19        0.506920       0.005240       -16.862724        1.288591   \n",
      "20        0.457434       0.005305       -13.544987        1.050892   \n",
      "21        0.413011       0.005365       -10.856432        0.853781   \n",
      "22        0.373158       0.005444        -8.678404        0.700423   \n",
      "23        0.337447       0.005559        -6.914261        0.576030   \n",
      "24        0.305424       0.005656        -5.483036        0.473981   \n",
      "25        0.276790       0.005757        -4.324265        0.394565   \n",
      "26        0.251217       0.005876        -3.385702        0.329986   \n",
      "27        0.228391       0.005970        -2.624776        0.277928   \n",
      "28        0.208038       0.006148        -2.007404        0.237156   \n",
      "29        0.189997       0.006306        -1.508371        0.204718   \n",
      "..             ...            ...              ...             ...   \n",
      "51        0.074930       0.007699         0.608669        0.070660   \n",
      "52        0.074546       0.007656         0.612696        0.069795   \n",
      "53        0.074247       0.007633         0.615812        0.069229   \n",
      "54        0.073992       0.007607         0.618456        0.068696   \n",
      "55        0.073812       0.007570         0.620332        0.068156   \n",
      "56        0.073641       0.007536         0.622110        0.067610   \n",
      "57        0.073475       0.007527         0.623819        0.067328   \n",
      "58        0.073377       0.007502         0.624826        0.067012   \n",
      "59        0.073312       0.007471         0.625523        0.066583   \n",
      "60        0.073244       0.007438         0.626242        0.066158   \n",
      "61        0.073187       0.007409         0.626837        0.065841   \n",
      "62        0.073138       0.007392         0.627345        0.065630   \n",
      "63        0.073098       0.007378         0.627770        0.065363   \n",
      "64        0.073061       0.007387         0.628145        0.065402   \n",
      "65        0.073042       0.007365         0.628344        0.065162   \n",
      "66        0.072999       0.007361         0.628790        0.065091   \n",
      "67        0.073002       0.007334         0.628785        0.064759   \n",
      "68        0.072996       0.007320         0.628865        0.064583   \n",
      "69        0.072996       0.007312         0.628870        0.064446   \n",
      "70        0.072988       0.007303         0.628969        0.064313   \n",
      "71        0.072986       0.007288         0.629005        0.064135   \n",
      "72        0.073003       0.007288         0.628830        0.064135   \n",
      "73        0.073008       0.007274         0.628794        0.063975   \n",
      "74        0.072998       0.007279         0.628893        0.064001   \n",
      "75        0.072989       0.007275         0.628990        0.063958   \n",
      "76        0.072985       0.007280         0.629039        0.063971   \n",
      "77        0.072986       0.007278         0.629037        0.063872   \n",
      "78        0.072983       0.007276         0.629078        0.063789   \n",
      "79        0.072982       0.007269         0.629103        0.063715   \n",
      "80        0.072980       0.007269         0.629118        0.063729   \n",
      "\n",
      "    train-rmse-mean  train-rmse-std  train-score-mean  train-score-std  \n",
      "0          3.704797        0.000669       -945.457919         7.679207  \n",
      "1          3.334762        0.000603       -765.835343         6.220730  \n",
      "2          3.001738        0.000544       -620.323864         5.039203  \n",
      "3          2.702030        0.000491       -502.445635         4.082061  \n",
      "4          2.432280        0.000444       -406.942965         3.306754  \n",
      "5          2.189513        0.000401       -329.572855         2.678653  \n",
      "6          1.971034        0.000363       -266.892565         2.169806  \n",
      "7          1.774422        0.000329       -216.113026         1.757563  \n",
      "8          1.597493        0.000299       -174.974709         1.423598  \n",
      "9          1.438275        0.000271       -141.644945         1.152937  \n",
      "10         1.295004        0.000250       -114.641730         0.933828  \n",
      "11         1.166089        0.000230        -92.763910         0.755829  \n",
      "12         1.050103        0.000213        -75.038900         0.612280  \n",
      "13         0.945761        0.000198        -60.678704         0.495780  \n",
      "14         0.851905        0.000187        -49.044302         0.401488  \n",
      "15         0.767494        0.000179        -39.618374         0.325061  \n",
      "16         0.691592        0.000173        -31.981556         0.262921  \n",
      "17         0.623352        0.000170        -25.794072         0.212721  \n",
      "18         0.562016        0.000169        -20.780563         0.172111  \n",
      "19         0.506913        0.000173        -16.718930         0.139095  \n",
      "20         0.457420        0.000179        -13.427823         0.112356  \n",
      "21         0.412994        0.000188        -10.761400         0.090722  \n",
      "22         0.373136        0.000198         -8.600742         0.073264  \n",
      "23         0.337405        0.000212         -6.850070         0.059073  \n",
      "24         0.305399        0.000227         -5.431406         0.047656  \n",
      "25         0.276760        0.000243         -4.281731         0.038485  \n",
      "26         0.251173        0.000263         -3.350252         0.031033  \n",
      "27         0.228339        0.000283         -2.595249         0.025092  \n",
      "28         0.208004        0.000307         -1.983386         0.020344  \n",
      "29         0.189930        0.000338         -1.487453         0.016544  \n",
      "..              ...             ...               ...              ...  \n",
      "51         0.073903        0.000870          0.623375         0.007489  \n",
      "52         0.073449        0.000874          0.627988         0.007486  \n",
      "53         0.073083        0.000888          0.631691         0.007577  \n",
      "54         0.072770        0.000893          0.634834         0.007609  \n",
      "55         0.072496        0.000892          0.637573         0.007613  \n",
      "56         0.072274        0.000904          0.639788         0.007696  \n",
      "57         0.072080        0.000905          0.641722         0.007662  \n",
      "58         0.071907        0.000900          0.643445         0.007654  \n",
      "59         0.071773        0.000895          0.644764         0.007610  \n",
      "60         0.071651        0.000893          0.645974         0.007562  \n",
      "61         0.071550        0.000892          0.646973         0.007542  \n",
      "62         0.071460        0.000900          0.647860         0.007592  \n",
      "63         0.071376        0.000893          0.648691         0.007509  \n",
      "64         0.071288        0.000894          0.649548         0.007484  \n",
      "65         0.071202        0.000905          0.650393         0.007719  \n",
      "66         0.071134        0.000905          0.651065         0.007702  \n",
      "67         0.071069        0.000895          0.651700         0.007600  \n",
      "68         0.070999        0.000903          0.652382         0.007779  \n",
      "69         0.070926        0.000883          0.653093         0.007665  \n",
      "70         0.070879        0.000880          0.653561         0.007633  \n",
      "71         0.070827        0.000874          0.654071         0.007573  \n",
      "72         0.070758        0.000882          0.654736         0.007584  \n",
      "73         0.070709        0.000888          0.655217         0.007666  \n",
      "74         0.070643        0.000899          0.655857         0.007841  \n",
      "75         0.070606        0.000895          0.656216         0.007825  \n",
      "76         0.070564        0.000901          0.656626         0.007931  \n",
      "77         0.070502        0.000896          0.657227         0.007797  \n",
      "78         0.070470        0.000895          0.657543         0.007772  \n",
      "79         0.070429        0.000883          0.657945         0.007643  \n",
      "80         0.070397        0.000881          0.658252         0.007591  \n",
      "\n",
      "[81 rows x 8 columns]\n",
      "num_rounds 81.000000\n",
      "\n",
      "Model Report\n",
      "R2 : 0.6581\n"
     ]
    }
   ],
   "source": [
    "xgb1 = XGBRegressor(\n",
    "                     learning_rate =0.1,\n",
    "                     n_estimators=500,\n",
    "                     max_depth=3,\n",
    "                     min_child_weight=0.5,\n",
    "                     gamma=0,\n",
    "                     #subsample=0.8,\n",
    "                     #colsample_bytree=0.8,\n",
    "                     objective= 'reg:linear',\n",
    "                     #base_score = y_mean,\n",
    "                     nthread=4,\n",
    "                     scale_pos_weight=1,\n",
    "                     seed=27)\n",
    "                     \n",
    "modelfit(xgb1,train_X.values, train_y, predictors,test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\softwareinstallion\\anaconda2\\lib\\site-packages\\sklearn\\model_selection\\_search.py:667: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([mean: -0.00593, std: 0.00106, params: {'gamma': 0.5},\n",
       "  mean: -0.00617, std: 0.00110, params: {'gamma': 0.8},\n",
       "  mean: -0.00633, std: 0.00109, params: {'gamma': 1},\n",
       "  mean: -0.00658, std: 0.00102, params: {'gamma': 1.2},\n",
       "  mean: -0.00683, std: 0.00106, params: {'gamma': 1.5}],\n",
       " {'gamma': 0.5},\n",
       " -0.0059342727050987424)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#continue increase the power of the model \n",
    "param_test = {\n",
    " 'gamma':[0,0.5,0.8,1]\n",
    "}\n",
    "gsearch = GridSearchCV(estimator = XGBRegressor( \n",
    "    learning_rate =0.1,\n",
    "    n_estimators=81,\n",
    "    max_depth=3,\n",
    "    min_child_weight=0.5,\n",
    "    gamma=0,\n",
    "    objective= 'reg:linear',\n",
    "    nthread=3,\n",
    "    scale_pos_weight=1,\n",
    "    seed=27), \n",
    "param_grid = param_test, scoring='neg_mean_squared_error',n_jobs=4,iid=False, cv=5)\n",
    "gsearch.fit(train_X.values,train_y)\n",
    "gsearch.grid_scores_, gsearch.best_params_, gsearch.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    test-rmse-mean  test-rmse-std  test-score-mean  test-score-std  \\\n",
      "0         3.704793       0.006541      -953.348681       70.073531   \n",
      "1         3.334759       0.006440      -772.228560       56.787973   \n",
      "2         3.001736       0.006368      -625.503876       46.024803   \n",
      "3         2.702028       0.006322      -506.642790       37.305061   \n",
      "4         2.432281       0.006054      -410.342282       30.204074   \n",
      "5         2.189516       0.005828      -332.325931       24.454040   \n",
      "6         1.971039       0.005640      -269.122202       19.798158   \n",
      "7         1.774428       0.005483      -217.918702       16.028442   \n",
      "8         1.597501       0.005355      -176.437004       12.976398   \n",
      "9         1.438266       0.005232      -142.825630       10.506010   \n",
      "10        1.295000       0.005135      -115.598172        8.503859   \n",
      "11        1.166086       0.005094       -93.537989        6.883040   \n",
      "12        1.050095       0.005061       -75.664571        5.571442   \n",
      "13        0.945754       0.005045       -61.185060        4.511047   \n",
      "14        0.851920       0.005060       -49.456893        3.657280   \n",
      "15        0.767530       0.005070       -39.955057        2.967563   \n",
      "16        0.691623       0.005120       -32.254291        2.408881   \n",
      "17        0.623349       0.005119       -26.011975        1.950329   \n",
      "18        0.562029       0.005212       -20.958257        1.584270   \n",
      "19        0.506920       0.005240       -16.862724        1.288591   \n",
      "20        0.457434       0.005305       -13.544987        1.050892   \n",
      "21        0.413011       0.005365       -10.856432        0.853781   \n",
      "22        0.373158       0.005444        -8.678404        0.700423   \n",
      "23        0.337447       0.005559        -6.914261        0.576030   \n",
      "24        0.305424       0.005656        -5.483036        0.473981   \n",
      "25        0.276790       0.005757        -4.324265        0.394565   \n",
      "26        0.251217       0.005876        -3.385702        0.329986   \n",
      "27        0.228391       0.005970        -2.624776        0.277928   \n",
      "28        0.208038       0.006148        -2.007404        0.237156   \n",
      "29        0.189997       0.006306        -1.508371        0.204718   \n",
      "..             ...            ...              ...             ...   \n",
      "51        0.074930       0.007699         0.608669        0.070660   \n",
      "52        0.074546       0.007656         0.612696        0.069795   \n",
      "53        0.074247       0.007633         0.615812        0.069229   \n",
      "54        0.073992       0.007607         0.618456        0.068696   \n",
      "55        0.073812       0.007570         0.620332        0.068156   \n",
      "56        0.073641       0.007536         0.622110        0.067610   \n",
      "57        0.073475       0.007527         0.623819        0.067328   \n",
      "58        0.073377       0.007502         0.624826        0.067012   \n",
      "59        0.073312       0.007471         0.625523        0.066583   \n",
      "60        0.073244       0.007438         0.626242        0.066158   \n",
      "61        0.073187       0.007409         0.626837        0.065841   \n",
      "62        0.073138       0.007392         0.627345        0.065630   \n",
      "63        0.073098       0.007378         0.627770        0.065363   \n",
      "64        0.073061       0.007387         0.628145        0.065402   \n",
      "65        0.073042       0.007365         0.628344        0.065162   \n",
      "66        0.072999       0.007361         0.628790        0.065091   \n",
      "67        0.073002       0.007334         0.628785        0.064759   \n",
      "68        0.072996       0.007320         0.628865        0.064583   \n",
      "69        0.072996       0.007312         0.628870        0.064446   \n",
      "70        0.072988       0.007303         0.628969        0.064313   \n",
      "71        0.072986       0.007288         0.629005        0.064135   \n",
      "72        0.073003       0.007288         0.628830        0.064135   \n",
      "73        0.073008       0.007274         0.628794        0.063975   \n",
      "74        0.072998       0.007279         0.628893        0.064001   \n",
      "75        0.072989       0.007275         0.628990        0.063958   \n",
      "76        0.072985       0.007280         0.629039        0.063971   \n",
      "77        0.072986       0.007278         0.629037        0.063872   \n",
      "78        0.072983       0.007276         0.629078        0.063789   \n",
      "79        0.072982       0.007269         0.629103        0.063715   \n",
      "80        0.072980       0.007269         0.629118        0.063729   \n",
      "\n",
      "    train-rmse-mean  train-rmse-std  train-score-mean  train-score-std  \n",
      "0          3.704797        0.000669       -945.457919         7.679207  \n",
      "1          3.334762        0.000603       -765.835343         6.220730  \n",
      "2          3.001738        0.000544       -620.323864         5.039203  \n",
      "3          2.702030        0.000491       -502.445635         4.082061  \n",
      "4          2.432280        0.000444       -406.942965         3.306754  \n",
      "5          2.189513        0.000401       -329.572855         2.678653  \n",
      "6          1.971034        0.000363       -266.892565         2.169806  \n",
      "7          1.774422        0.000329       -216.113026         1.757563  \n",
      "8          1.597493        0.000299       -174.974709         1.423598  \n",
      "9          1.438275        0.000271       -141.644945         1.152937  \n",
      "10         1.295004        0.000250       -114.641730         0.933828  \n",
      "11         1.166089        0.000230        -92.763910         0.755829  \n",
      "12         1.050103        0.000213        -75.038900         0.612280  \n",
      "13         0.945761        0.000198        -60.678704         0.495780  \n",
      "14         0.851905        0.000187        -49.044302         0.401488  \n",
      "15         0.767494        0.000179        -39.618374         0.325061  \n",
      "16         0.691592        0.000173        -31.981556         0.262921  \n",
      "17         0.623352        0.000170        -25.794072         0.212721  \n",
      "18         0.562016        0.000169        -20.780563         0.172111  \n",
      "19         0.506913        0.000173        -16.718930         0.139095  \n",
      "20         0.457420        0.000179        -13.427823         0.112356  \n",
      "21         0.412994        0.000188        -10.761400         0.090722  \n",
      "22         0.373136        0.000198         -8.600742         0.073264  \n",
      "23         0.337405        0.000212         -6.850070         0.059073  \n",
      "24         0.305399        0.000227         -5.431406         0.047656  \n",
      "25         0.276760        0.000243         -4.281731         0.038485  \n",
      "26         0.251173        0.000263         -3.350252         0.031033  \n",
      "27         0.228339        0.000283         -2.595249         0.025092  \n",
      "28         0.208004        0.000307         -1.983386         0.020344  \n",
      "29         0.189930        0.000338         -1.487453         0.016544  \n",
      "..              ...             ...               ...              ...  \n",
      "51         0.073903        0.000870          0.623375         0.007489  \n",
      "52         0.073449        0.000874          0.627988         0.007486  \n",
      "53         0.073083        0.000888          0.631691         0.007577  \n",
      "54         0.072770        0.000893          0.634834         0.007609  \n",
      "55         0.072496        0.000892          0.637573         0.007613  \n",
      "56         0.072274        0.000904          0.639788         0.007696  \n",
      "57         0.072080        0.000905          0.641722         0.007662  \n",
      "58         0.071907        0.000900          0.643445         0.007654  \n",
      "59         0.071773        0.000895          0.644764         0.007610  \n",
      "60         0.071651        0.000893          0.645974         0.007562  \n",
      "61         0.071550        0.000892          0.646973         0.007542  \n",
      "62         0.071460        0.000900          0.647860         0.007592  \n",
      "63         0.071376        0.000893          0.648691         0.007509  \n",
      "64         0.071288        0.000894          0.649548         0.007484  \n",
      "65         0.071202        0.000905          0.650393         0.007719  \n",
      "66         0.071134        0.000905          0.651065         0.007702  \n",
      "67         0.071069        0.000895          0.651700         0.007600  \n",
      "68         0.070999        0.000903          0.652382         0.007779  \n",
      "69         0.070926        0.000883          0.653093         0.007665  \n",
      "70         0.070879        0.000880          0.653561         0.007633  \n",
      "71         0.070827        0.000874          0.654071         0.007573  \n",
      "72         0.070758        0.000882          0.654736         0.007584  \n",
      "73         0.070709        0.000888          0.655217         0.007666  \n",
      "74         0.070643        0.000899          0.655857         0.007841  \n",
      "75         0.070606        0.000895          0.656216         0.007825  \n",
      "76         0.070564        0.000901          0.656626         0.007931  \n",
      "77         0.070502        0.000896          0.657227         0.007797  \n",
      "78         0.070470        0.000895          0.657543         0.007772  \n",
      "79         0.070429        0.000883          0.657945         0.007643  \n",
      "80         0.070397        0.000881          0.658252         0.007591  \n",
      "\n",
      "[81 rows x 8 columns]\n",
      "num_rounds 81.000000\n",
      "\n",
      "Model Report\n",
      "R2 : 0.6581\n"
     ]
    }
   ],
   "source": [
    "xgb1 = XGBRegressor(\n",
    "                     learning_rate =0.1,\n",
    "                     n_estimators=500,\n",
    "                     max_depth=3,\n",
    "                     min_child_weight=0.5,\n",
    "                     gamma=0,\n",
    "                     #subsample=0.8,\n",
    "                     #colsample_bytree=0.8,\n",
    "                     objective= 'reg:linear',\n",
    "                     #base_score = y_mean,\n",
    "                     nthread=4,\n",
    "                     scale_pos_weight=1,\n",
    "                     seed=27)\n",
    "                     \n",
    "modelfit(xgb1,train_X.values, train_y, predictors,test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try stacking methods\n",
    "\n",
    "https://www.kaggle.com/arthurtok/introduction-to-ensembling-stacking-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Some useful parameters which will come in handy later on\n",
    "ntrain = train_X.shape[0]\n",
    "ntest = test_X.shape[0]\n",
    "SEED = 0 # for reproducibility\n",
    "NFOLDS = 5 # set folds for out-of-fold prediction\n",
    "kf = KFold(n_splits=NFOLDS, random_state=SEED).split(train_X)\n",
    "\n",
    "# Class to extend the Sklearn classifier\n",
    "class SklearnHelper(object):\n",
    "    def __init__(self, clf, seed=0, params=None):\n",
    "        #params['random_state'] = seed\n",
    "        self.clf = clf(**params)\n",
    "\n",
    "    def train(self, x_train, y_train):\n",
    "        self.clf.fit(x_train, y_train)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.clf.predict(x)\n",
    "    \n",
    "    def fit(self,x,y):\n",
    "        return self.clf.fit(x,y)\n",
    "    \n",
    "    def feature_importances(self,x,y):\n",
    "        print(self.clf.fit(x,y).feature_importances_)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_oof(clf, x_train, y_train, x_test):\n",
    "    oof_train = np.zeros((ntrain,))\n",
    "    oof_test = np.zeros((ntest,))\n",
    "    oof_test_skf = np.empty((NFOLDS, ntest))\n",
    "    # since kf is a generator object, and it can't be reused, so we should add this line in this function\n",
    "    kf = KFold(n_splits=NFOLDS, random_state=SEED).split(train_X)\n",
    "    for i,(train_index, test_index) in enumerate(kf):\n",
    "        x_tr = x_train[train_index]\n",
    "        y_tr = y_train[train_index]\n",
    "        x_te = x_train[test_index]\n",
    "\n",
    "        clf.train(x_tr, y_tr)\n",
    "\n",
    "        oof_train[test_index] = clf.predict(x_te)\n",
    "        oof_test[:] = oof_test_skf.mean(axis=0)\n",
    "        oof_test_skf[i, :] = clf.predict(x_test)\n",
    "\n",
    "    oof_test[:] = oof_test_skf.mean(axis=0)\n",
    "    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "rf_params = {\n",
    "    #'n_jobs': -1,\n",
    "    'n_estimators': 500,\n",
    "     #'warm_start': True, \n",
    "     #'max_features': 0.2,\n",
    "    #'max_depth': 6,\n",
    "    #'min_samples_leaf': 2,\n",
    "    #'max_features' : 'sqrt',\n",
    "    #'verbose': 0\n",
    "}\n",
    "svr_params={\n",
    "    'C':1,\n",
    "    'epsilon':0.1,\n",
    "    'kernel':'rbf',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create 5 objects that represent our 4 models\n",
    "rf = SklearnHelper(clf=RandomForestRegressor, seed=SEED, params=rf_params)\n",
    "#et = SklearnHelper(clf=ExtraTreesClassifier, seed=SEED, params=et_params)\n",
    "#ada = SklearnHelper(clf=AdaBoostClassifier, seed=SEED, params=ada_params)\n",
    "#gb = SklearnHelper(clf=GradientBoostingClassifier, seed=SEED, params=gb_params)\n",
    "#svr = SklearnHelper(clf=SVR, seed=SEED,params = svr_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create Numpy arrays of train, test and target ( Survived) dataframes to feed into our models\n",
    "y_train = train_y.ravel()\n",
    "train = train_X[predictors]\n",
    "x_train = train.values # Creates an array of the train data\n",
    "x_test = test_X[predictors].values # Creats an array of the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training is complete\n"
     ]
    }
   ],
   "source": [
    "# Create our OOF train and test predictions. These base results will be used as new features\n",
    "\n",
    "rf_oof_train, rf_oof_test = get_oof(rf,x_train, y_train, x_test) # Random Forest\n",
    "#svr_oof_train, svr_oof_test = get_oof(svr,x_train, y_train, x_test) # Support Vector Classifier\n",
    "\n",
    "print(\"Training is complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4209, 2)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_predictions_train = pd.DataFrame( {'RandomForest': rf_oof_train.ravel(),\n",
    "      'SVR': svr_oof_train.ravel()\n",
    "    })\n",
    "base_predictions_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train = np.concatenate(( rf_oof_train,svr_oof_train), axis=1)\n",
    "x_test = np.concatenate(( rf_oof_test,svr_oof_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99,\n",
       " 100,\n",
       " 101,\n",
       " 102,\n",
       " 103,\n",
       " 104,\n",
       " 105,\n",
       " 106,\n",
       " 107,\n",
       " 108,\n",
       " 109,\n",
       " 110,\n",
       " 111,\n",
       " 112,\n",
       " 113,\n",
       " 114,\n",
       " 115,\n",
       " 116,\n",
       " 117,\n",
       " 118,\n",
       " 119,\n",
       " 120,\n",
       " 121,\n",
       " 122,\n",
       " 123,\n",
       " 124,\n",
       " 125,\n",
       " 126,\n",
       " 127,\n",
       " 128,\n",
       " 129,\n",
       " 130,\n",
       " 131,\n",
       " 132,\n",
       " 133,\n",
       " 134,\n",
       " 135,\n",
       " 136,\n",
       " 137,\n",
       " 138,\n",
       " 139,\n",
       " 140,\n",
       " 141,\n",
       " 142,\n",
       " 143,\n",
       " 144,\n",
       " 145,\n",
       " 146,\n",
       " 147,\n",
       " 148,\n",
       " 149,\n",
       " 150,\n",
       " 151,\n",
       " 152,\n",
       " 153,\n",
       " 154,\n",
       " 155,\n",
       " 156,\n",
       " 157,\n",
       " 158,\n",
       " 159,\n",
       " 160,\n",
       " 161,\n",
       " 162,\n",
       " 163,\n",
       " 164,\n",
       " 165,\n",
       " 166,\n",
       " 167,\n",
       " 168,\n",
       " 169,\n",
       " 170,\n",
       " 171,\n",
       " 172,\n",
       " 173,\n",
       " 174,\n",
       " 175,\n",
       " 176,\n",
       " 177,\n",
       " 178,\n",
       " 179,\n",
       " 180,\n",
       " 181,\n",
       " 182,\n",
       " 183,\n",
       " 184,\n",
       " 185,\n",
       " 186,\n",
       " 187,\n",
       " 188,\n",
       " 189,\n",
       " 190,\n",
       " 191,\n",
       " 192,\n",
       " 193,\n",
       " 194,\n",
       " 195,\n",
       " 196,\n",
       " 197,\n",
       " 198,\n",
       " 199,\n",
       " 200,\n",
       " 201,\n",
       " 202,\n",
       " 203,\n",
       " 204,\n",
       " 205,\n",
       " 206,\n",
       " 207,\n",
       " 208,\n",
       " 209,\n",
       " 210,\n",
       " 211,\n",
       " 212,\n",
       " 213,\n",
       " 214,\n",
       " 215,\n",
       " 216,\n",
       " 217,\n",
       " 218,\n",
       " 219,\n",
       " 220,\n",
       " 221,\n",
       " 222,\n",
       " 223,\n",
       " 224,\n",
       " 225,\n",
       " 226,\n",
       " 227,\n",
       " 228,\n",
       " 229,\n",
       " 230,\n",
       " 231,\n",
       " 232,\n",
       " 233,\n",
       " 234,\n",
       " 235,\n",
       " 236,\n",
       " 237,\n",
       " 238,\n",
       " 239,\n",
       " 240,\n",
       " 241,\n",
       " 242,\n",
       " 243,\n",
       " 244,\n",
       " 245,\n",
       " 246,\n",
       " 247,\n",
       " 248,\n",
       " 249,\n",
       " 250,\n",
       " 251,\n",
       " 252,\n",
       " 253,\n",
       " 254,\n",
       " 255,\n",
       " 256,\n",
       " 257,\n",
       " 258,\n",
       " 259,\n",
       " 260,\n",
       " 261,\n",
       " 262,\n",
       " 263,\n",
       " 264,\n",
       " 265,\n",
       " 266,\n",
       " 267,\n",
       " 268,\n",
       " 269,\n",
       " 270,\n",
       " 271,\n",
       " 272,\n",
       " 273,\n",
       " 274,\n",
       " 275,\n",
       " 276,\n",
       " 277,\n",
       " 278,\n",
       " 279,\n",
       " 280,\n",
       " 281,\n",
       " 282,\n",
       " 283,\n",
       " 284,\n",
       " 285,\n",
       " 286,\n",
       " 287,\n",
       " 288,\n",
       " 289,\n",
       " 290,\n",
       " 291,\n",
       " 292,\n",
       " 293,\n",
       " 294,\n",
       " 295,\n",
       " 296,\n",
       " 297,\n",
       " 298,\n",
       " 299,\n",
       " 300,\n",
       " 301,\n",
       " 302,\n",
       " 303,\n",
       " 304,\n",
       " 305,\n",
       " 306,\n",
       " 307,\n",
       " 308,\n",
       " 309,\n",
       " 310,\n",
       " 311,\n",
       " 312,\n",
       " 313,\n",
       " 314,\n",
       " 315,\n",
       " 316,\n",
       " 317,\n",
       " 318,\n",
       " 319,\n",
       " 320,\n",
       " 321,\n",
       " 322,\n",
       " 323,\n",
       " 324,\n",
       " 325,\n",
       " 326,\n",
       " 327,\n",
       " 328,\n",
       " 329,\n",
       " 330,\n",
       " 331,\n",
       " 332,\n",
       " 333,\n",
       " 334,\n",
       " 335,\n",
       " 336,\n",
       " 337,\n",
       " 338,\n",
       " 339,\n",
       " 340,\n",
       " 341,\n",
       " 342,\n",
       " 343,\n",
       " 344,\n",
       " 345,\n",
       " 346,\n",
       " 347,\n",
       " 348,\n",
       " 349,\n",
       " 350,\n",
       " 351,\n",
       " 352,\n",
       " 353,\n",
       " 354,\n",
       " 355,\n",
       " 356,\n",
       " 357,\n",
       " 358,\n",
       " 359,\n",
       " 360,\n",
       " 361,\n",
       " 362,\n",
       " 363,\n",
       " 364,\n",
       " 365,\n",
       " 366,\n",
       " 367,\n",
       " 368,\n",
       " 369,\n",
       " 370,\n",
       " 371,\n",
       " 372,\n",
       " 373,\n",
       " 374,\n",
       " 375,\n",
       " 376,\n",
       " 377,\n",
       " 378,\n",
       " 379,\n",
       " 380,\n",
       " 381,\n",
       " 382,\n",
       " 383,\n",
       " 384,\n",
       " 385,\n",
       " 386,\n",
       " 387,\n",
       " 388,\n",
       " 389,\n",
       " 390,\n",
       " 391,\n",
       " 392,\n",
       " 393,\n",
       " 394,\n",
       " 395,\n",
       " 396,\n",
       " 397,\n",
       " 398,\n",
       " 399,\n",
       " 400,\n",
       " 401,\n",
       " 402,\n",
       " 403,\n",
       " 404,\n",
       " 405,\n",
       " 406,\n",
       " 407,\n",
       " 408,\n",
       " 409,\n",
       " 410,\n",
       " 411,\n",
       " 412,\n",
       " 413,\n",
       " 414,\n",
       " 415,\n",
       " 416,\n",
       " 417,\n",
       " 418,\n",
       " 419,\n",
       " 420,\n",
       " 421,\n",
       " 422,\n",
       " 423,\n",
       " 424,\n",
       " 425,\n",
       " 426,\n",
       " 427,\n",
       " 428,\n",
       " 429,\n",
       " 430,\n",
       " 431,\n",
       " 432,\n",
       " 433,\n",
       " 434,\n",
       " 435,\n",
       " 436,\n",
       " 437,\n",
       " 438,\n",
       " 439,\n",
       " 440,\n",
       " 441,\n",
       " 442,\n",
       " 443,\n",
       " 444,\n",
       " 445,\n",
       " 446,\n",
       " 447,\n",
       " 448,\n",
       " 449,\n",
       " 450,\n",
       " 451,\n",
       " 452,\n",
       " 453,\n",
       " 454,\n",
       " 455,\n",
       " 456,\n",
       " 457,\n",
       " 458,\n",
       " 459,\n",
       " 460,\n",
       " 461,\n",
       " 462,\n",
       " 463,\n",
       " 464,\n",
       " 465,\n",
       " 466,\n",
       " 467,\n",
       " 468,\n",
       " 469,\n",
       " 470,\n",
       " 471,\n",
       " 472,\n",
       " 473,\n",
       " 474,\n",
       " 475,\n",
       " 476,\n",
       " 477,\n",
       " 478,\n",
       " 479,\n",
       " 480,\n",
       " 481,\n",
       " 482,\n",
       " 483,\n",
       " 484,\n",
       " 485,\n",
       " 486,\n",
       " 487,\n",
       " 488,\n",
       " 489,\n",
       " 490,\n",
       " 491,\n",
       " 492,\n",
       " 493,\n",
       " 494,\n",
       " 495,\n",
       " 496,\n",
       " 497,\n",
       " 498,\n",
       " 499,\n",
       " 500,\n",
       " 501,\n",
       " 502,\n",
       " 503,\n",
       " 504,\n",
       " 505,\n",
       " 506,\n",
       " 507,\n",
       " 508,\n",
       " 509,\n",
       " 510,\n",
       " 511,\n",
       " 512,\n",
       " 513,\n",
       " 514,\n",
       " 515,\n",
       " 516,\n",
       " 517,\n",
       " 518,\n",
       " 519,\n",
       " 520,\n",
       " 521,\n",
       " 522,\n",
       " 523,\n",
       " 524,\n",
       " 525,\n",
       " 526,\n",
       " 527,\n",
       " 528,\n",
       " 529,\n",
       " 530,\n",
       " 531,\n",
       " 532,\n",
       " 533,\n",
       " 534,\n",
       " 535,\n",
       " 536,\n",
       " 537,\n",
       " 538,\n",
       " 539,\n",
       " 540,\n",
       " 541,\n",
       " 542,\n",
       " 543,\n",
       " 544,\n",
       " 545,\n",
       " 546,\n",
       " 547,\n",
       " 548,\n",
       " 549,\n",
       " 550,\n",
       " 551,\n",
       " 552,\n",
       " 553,\n",
       " 554,\n",
       " 555,\n",
       " 556,\n",
       " 557,\n",
       " 558,\n",
       " 559,\n",
       " 560,\n",
       " 561,\n",
       " 562,\n",
       " 563,\n",
       " 564,\n",
       " 565,\n",
       " 566,\n",
       " 567,\n",
       " 568,\n",
       " 569,\n",
       " 570,\n",
       " 571,\n",
       " 572,\n",
       " 573,\n",
       " 574,\n",
       " 575,\n",
       " 576,\n",
       " 577,\n",
       " 578,\n",
       " 579,\n",
       " 580,\n",
       " 581,\n",
       " 582,\n",
       " 583,\n",
       " 584,\n",
       " 585,\n",
       " 586,\n",
       " 587,\n",
       " 588,\n",
       " 589,\n",
       " 590,\n",
       " 591,\n",
       " 592,\n",
       " 593,\n",
       " 594,\n",
       " 595,\n",
       " 596,\n",
       " 597,\n",
       " 598,\n",
       " 599,\n",
       " 600,\n",
       " 601,\n",
       " 602,\n",
       " 603,\n",
       " 604,\n",
       " 605,\n",
       " 606,\n",
       " 607,\n",
       " 608,\n",
       " 609,\n",
       " 610,\n",
       " 611,\n",
       " 612,\n",
       " 613,\n",
       " 614,\n",
       " 615,\n",
       " 616,\n",
       " 617,\n",
       " 618,\n",
       " 619,\n",
       " 620,\n",
       " 621,\n",
       " 622,\n",
       " 623,\n",
       " 624,\n",
       " 625,\n",
       " 626,\n",
       " 627,\n",
       " 628,\n",
       " 629,\n",
       " 630,\n",
       " 631,\n",
       " 632,\n",
       " 633,\n",
       " 634,\n",
       " 635,\n",
       " 636,\n",
       " 637,\n",
       " 638,\n",
       " 639,\n",
       " 640,\n",
       " 641,\n",
       " 642,\n",
       " 643,\n",
       " 644,\n",
       " 645,\n",
       " 646,\n",
       " 647,\n",
       " 648,\n",
       " 649,\n",
       " 650,\n",
       " 651,\n",
       " 652,\n",
       " 653,\n",
       " 654,\n",
       " 655,\n",
       " 656,\n",
       " 657,\n",
       " 658,\n",
       " 659,\n",
       " 660,\n",
       " 661,\n",
       " 662,\n",
       " 663,\n",
       " 664,\n",
       " 665,\n",
       " 666,\n",
       " 667,\n",
       " 668,\n",
       " 669,\n",
       " 670,\n",
       " 671,\n",
       " 672,\n",
       " 673,\n",
       " 674,\n",
       " 675,\n",
       " 676,\n",
       " 677,\n",
       " 678,\n",
       " 679,\n",
       " 680,\n",
       " 681,\n",
       " 682,\n",
       " 683,\n",
       " 684,\n",
       " 685,\n",
       " 686,\n",
       " 687,\n",
       " 688,\n",
       " 689,\n",
       " 690,\n",
       " 691,\n",
       " 692,\n",
       " 693,\n",
       " 694,\n",
       " 695,\n",
       " 696,\n",
       " 697,\n",
       " 698,\n",
       " 699,\n",
       " 700,\n",
       " 701,\n",
       " 702,\n",
       " 703,\n",
       " 704,\n",
       " 705,\n",
       " 706,\n",
       " 707,\n",
       " 708,\n",
       " 709,\n",
       " 710,\n",
       " 711,\n",
       " 712,\n",
       " 713,\n",
       " 714,\n",
       " 715,\n",
       " 716,\n",
       " 717,\n",
       " 718,\n",
       " 719,\n",
       " 720,\n",
       " 721,\n",
       " 722,\n",
       " 723,\n",
       " 724,\n",
       " 725,\n",
       " 726,\n",
       " 727,\n",
       " 728,\n",
       " 729,\n",
       " 730,\n",
       " 731,\n",
       " 732,\n",
       " 733,\n",
       " 734,\n",
       " 735,\n",
       " 736,\n",
       " 737,\n",
       " 738,\n",
       " 739,\n",
       " 740,\n",
       " 741,\n",
       " 742,\n",
       " 743,\n",
       " 744,\n",
       " 745,\n",
       " 746,\n",
       " 747,\n",
       " 748,\n",
       " 749,\n",
       " 750,\n",
       " 751,\n",
       " 752,\n",
       " 753,\n",
       " 754,\n",
       " 755,\n",
       " 756,\n",
       " 757,\n",
       " 758,\n",
       " 759,\n",
       " 760,\n",
       " 761,\n",
       " 762,\n",
       " 763,\n",
       " 764,\n",
       " 765,\n",
       " 766,\n",
       " 767,\n",
       " 768,\n",
       " 769,\n",
       " 770,\n",
       " 771,\n",
       " 772,\n",
       " 773,\n",
       " 774,\n",
       " 775,\n",
       " 776,\n",
       " 777,\n",
       " 778,\n",
       " 779,\n",
       " 780,\n",
       " 781,\n",
       " 782,\n",
       " 783,\n",
       " 784,\n",
       " 785,\n",
       " 786,\n",
       " 787,\n",
       " 788,\n",
       " 789,\n",
       " 790,\n",
       " 791,\n",
       " 792,\n",
       " 793,\n",
       " 794,\n",
       " 795,\n",
       " 796,\n",
       " 797,\n",
       " 798,\n",
       " 799,\n",
       " 800,\n",
       " 801,\n",
       " 802,\n",
       " 803,\n",
       " 804,\n",
       " 805,\n",
       " 806,\n",
       " 807,\n",
       " 808,\n",
       " 809,\n",
       " 810,\n",
       " 811,\n",
       " 812,\n",
       " 813,\n",
       " 814,\n",
       " 815,\n",
       " 816,\n",
       " 817,\n",
       " 818,\n",
       " 819,\n",
       " 820,\n",
       " 821,\n",
       " 822,\n",
       " 823,\n",
       " 824,\n",
       " 825,\n",
       " 826,\n",
       " 827,\n",
       " 828,\n",
       " 829,\n",
       " 830,\n",
       " 831,\n",
       " 832,\n",
       " 833,\n",
       " 834,\n",
       " 835,\n",
       " 836,\n",
       " 837,\n",
       " 838,\n",
       " 839,\n",
       " 840,\n",
       " 841,\n",
       " 842,\n",
       " 843,\n",
       " 844,\n",
       " 845,\n",
       " 846,\n",
       " 847,\n",
       " 848,\n",
       " 849,\n",
       " 850,\n",
       " 851,\n",
       " 852,\n",
       " 853,\n",
       " 854,\n",
       " 855,\n",
       " 856,\n",
       " 857,\n",
       " 858,\n",
       " 859,\n",
       " 860,\n",
       " 861,\n",
       " 862,\n",
       " 863,\n",
       " 864,\n",
       " 865,\n",
       " 866,\n",
       " 867,\n",
       " 868,\n",
       " 869,\n",
       " 870,\n",
       " 871,\n",
       " 872,\n",
       " 873,\n",
       " 874,\n",
       " 875,\n",
       " 876,\n",
       " 877,\n",
       " 878,\n",
       " 879,\n",
       " 880,\n",
       " 881,\n",
       " 882,\n",
       " 883,\n",
       " 884,\n",
       " 885,\n",
       " 886,\n",
       " 887,\n",
       " 888,\n",
       " 889,\n",
       " 890,\n",
       " 891,\n",
       " 892,\n",
       " 893,\n",
       " 894,\n",
       " 895,\n",
       " 896,\n",
       " 897,\n",
       " 898,\n",
       " 899,\n",
       " 900,\n",
       " 901,\n",
       " 902,\n",
       " 903,\n",
       " 904,\n",
       " 905,\n",
       " 906,\n",
       " 907,\n",
       " 908,\n",
       " 909,\n",
       " 910,\n",
       " 911,\n",
       " 912,\n",
       " 913,\n",
       " 914,\n",
       " 915,\n",
       " 916,\n",
       " 917,\n",
       " 918,\n",
       " 919,\n",
       " 920,\n",
       " 921,\n",
       " 922,\n",
       " 923,\n",
       " 924,\n",
       " 925,\n",
       " 926,\n",
       " 927,\n",
       " 928,\n",
       " 929,\n",
       " 930,\n",
       " 931,\n",
       " 932,\n",
       " 933,\n",
       " 934,\n",
       " 935,\n",
       " 936,\n",
       " 937,\n",
       " 938,\n",
       " 939,\n",
       " 940,\n",
       " 941,\n",
       " 942,\n",
       " 943,\n",
       " 944,\n",
       " 945,\n",
       " 946,\n",
       " 947,\n",
       " 948,\n",
       " 949,\n",
       " 950,\n",
       " 951,\n",
       " 952,\n",
       " 953,\n",
       " 954,\n",
       " 955,\n",
       " 956,\n",
       " 957,\n",
       " 958,\n",
       " 959,\n",
       " 960,\n",
       " 961,\n",
       " 962,\n",
       " 963,\n",
       " 964,\n",
       " 965,\n",
       " 966,\n",
       " 967,\n",
       " 968,\n",
       " 969,\n",
       " 970,\n",
       " 971,\n",
       " 972,\n",
       " 973,\n",
       " 974,\n",
       " 975,\n",
       " 976,\n",
       " 977,\n",
       " 978,\n",
       " 979,\n",
       " 980,\n",
       " 981,\n",
       " 982,\n",
       " 983,\n",
       " 984,\n",
       " 985,\n",
       " 986,\n",
       " 987,\n",
       " 988,\n",
       " 989,\n",
       " 990,\n",
       " 991,\n",
       " 992,\n",
       " 993,\n",
       " 994,\n",
       " 995,\n",
       " 996,\n",
       " 997,\n",
       " 998,\n",
       " 999,\n",
       " ...]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(test_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xgb1._Booster.save_model('tst.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_score': 0.5,\n",
       " 'colsample_bylevel': 1,\n",
       " 'colsample_bytree': 0.8,\n",
       " 'gamma': 0,\n",
       " 'learning_rate': 0.1,\n",
       " 'max_delta_step': 0,\n",
       " 'max_depth': 5,\n",
       " 'min_child_weight': 1,\n",
       " 'missing': None,\n",
       " 'n_estimators': 56,\n",
       " 'nthread': 4,\n",
       " 'objective': 'reg:linear',\n",
       " 'reg_alpha': 0,\n",
       " 'reg_lambda': 1,\n",
       " 'scale_pos_weight': 1,\n",
       " 'seed': 27,\n",
       " 'silent': True,\n",
       " 'subsample': 0.8}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb1.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xgb2 = XGBRegressor()\n",
    "#booster = Booster()\n",
    "#xgb2._Booster = \n",
    "from xgboost.sklearn import Booster\n",
    "booster = Booster()\n",
    "booster.load_model('tst.model')\n",
    "xgb2._Booster = booster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import  pickle\n",
    "pickle.dump(xgb1,open('xgb1.model','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_score': 0.5,\n",
       " 'colsample_bylevel': 1,\n",
       " 'colsample_bytree': 0.8,\n",
       " 'gamma': 0,\n",
       " 'learning_rate': 0.1,\n",
       " 'max_delta_step': 0,\n",
       " 'max_depth': 5,\n",
       " 'min_child_weight': 1,\n",
       " 'missing': nan,\n",
       " 'n_estimators': 56,\n",
       " 'nthread': 4,\n",
       " 'objective': 'reg:linear',\n",
       " 'reg_alpha': 0,\n",
       " 'reg_lambda': 1,\n",
       " 'scale_pos_weight': 1,\n",
       " 'seed': 27,\n",
       " 'silent': True,\n",
       " 'subsample': 0.8}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb3 = pickle.load(open('xgb1.model','rb'))\n",
    "xgb3.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.array([1,2,3,4,6])\n",
    "b=a.ravel()\n",
    "print(b.any())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
